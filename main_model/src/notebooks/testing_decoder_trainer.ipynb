{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/gridsan/dgerber/thesis/thesis_dgerber'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(\"../../\")\n",
    "os.chdir(\"./../\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Processing /home/gridsan/dgerber/thesis/thesis_dgerber\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: e in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (1.4.5)\n",
      "Requirement already satisfied: matplotlib==3.7.1 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (3.7.1)\n",
      "Requirement already satisfied: numpy==1.23.5 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (1.23.5)\n",
      "Requirement already satisfied: PyMCubes==0.1.2 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (0.1.2)\n",
      "Requirement already satisfied: scikit_learn==1.1.3 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (1.1.3)\n",
      "Requirement already satisfied: tqdm==4.65.0 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (4.65.0)\n",
      "Requirement already satisfied: websockets==10.3 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (10.3)\n",
      "Requirement already satisfied: tensorboard==2.17.0 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (2.17.0)\n",
      "Requirement already satisfied: pygeodesic==0.1.9 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (0.1.9)\n",
      "Requirement already satisfied: trimesh==4.6.4 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (4.6.4)\n",
      "Requirement already satisfied: yacs==0.1.8 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (0.1.8)\n",
      "Requirement already satisfied: polyscope==2.3.0 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (2.3.0)\n",
      "Requirement already satisfied: pyglet==1.5.31 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (1.5.31)\n",
      "Requirement already satisfied: torch in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (2.3.1)\n",
      "Requirement already satisfied: torch-cluster in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (1.6.3+pt23cu121)\n",
      "Requirement already satisfied: torch-geometric in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (2.6.1)\n",
      "Requirement already satisfied: torch-scatter in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (2.1.2+pt23cu121)\n",
      "Requirement already satisfied: wandb in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (0.19.8)\n",
      "Requirement already satisfied: h5py==3.13.0 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (3.13.0)\n",
      "Requirement already satisfied: pyvrp==0.10.0 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from thesis_dgerber==0.1.0) (0.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from matplotlib==3.7.1->thesis_dgerber==0.1.0) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from matplotlib==3.7.1->thesis_dgerber==0.1.0) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from matplotlib==3.7.1->thesis_dgerber==0.1.0) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from matplotlib==3.7.1->thesis_dgerber==0.1.0) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from matplotlib==3.7.1->thesis_dgerber==0.1.0) (24.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from matplotlib==3.7.1->thesis_dgerber==0.1.0) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from matplotlib==3.7.1->thesis_dgerber==0.1.0) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from matplotlib==3.7.1->thesis_dgerber==0.1.0) (2.9.0)\n",
      "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from pyvrp==0.10.0->thesis_dgerber==0.1.0) (2.0.1)\n",
      "Requirement already satisfied: vrplib<2.0.0,>=1.4.0 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from pyvrp==0.10.0->thesis_dgerber==0.1.0) (1.5.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from scikit_learn==1.1.3->thesis_dgerber==0.1.0) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from scikit_learn==1.1.3->thesis_dgerber==0.1.0) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from scikit_learn==1.1.3->thesis_dgerber==0.1.0) (3.5.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from tensorboard==2.17.0->thesis_dgerber==0.1.0) (2.1.0)\n",
      "Requirement already satisfied: grpcio>=1.48.2 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from tensorboard==2.17.0->thesis_dgerber==0.1.0) (1.65.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from tensorboard==2.17.0->thesis_dgerber==0.1.0) (3.6)\n",
      "Requirement already satisfied: protobuf!=4.24.0,<5.0.0,>=3.19.6 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from tensorboard==2.17.0->thesis_dgerber==0.1.0) (4.25.6)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from tensorboard==2.17.0->thesis_dgerber==0.1.0) (69.5.1)\n",
      "Requirement already satisfied: six>1.9 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from tensorboard==2.17.0->thesis_dgerber==0.1.0) (1.16.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from tensorboard==2.17.0->thesis_dgerber==0.1.0) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from tensorboard==2.17.0->thesis_dgerber==0.1.0) (3.0.3)\n",
      "Requirement already satisfied: PyYAML in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from yacs==0.1.8->thesis_dgerber==0.1.0) (6.0.2)\n",
      "Requirement already satisfied: filelock in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch->thesis_dgerber==0.1.0) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->thesis_dgerber==0.1.0) (12.6.20)\n",
      "Requirement already satisfied: aiohttp in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch-geometric->thesis_dgerber==0.1.0) (3.10.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch-geometric->thesis_dgerber==0.1.0) (6.0.0)\n",
      "Requirement already satisfied: requests in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from torch-geometric->thesis_dgerber==0.1.0) (2.31.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from wandb->thesis_dgerber==0.1.0) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from wandb->thesis_dgerber==0.1.0) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from wandb->thesis_dgerber==0.1.0) (3.1.43)\n",
      "Requirement already satisfied: platformdirs in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from wandb->thesis_dgerber==0.1.0) (4.2.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from wandb->thesis_dgerber==0.1.0) (2.8.2)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from wandb->thesis_dgerber==0.1.0) (2.23.1)\n",
      "Requirement already satisfied: setproctitle in /home/gridsan/dgerber/.local/lib/python3.10/site-packages (from wandb->thesis_dgerber==0.1.0) (1.3.5)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb->thesis_dgerber==0.1.0) (4.0.11)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb->thesis_dgerber==0.1.0) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb->thesis_dgerber==0.1.0) (2.20.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from requests->torch-geometric->thesis_dgerber==0.1.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from requests->torch-geometric->thesis_dgerber==0.1.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from requests->torch-geometric->thesis_dgerber==0.1.0) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from requests->torch-geometric->thesis_dgerber==0.1.0) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard==2.17.0->thesis_dgerber==0.1.0) (2.1.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from aiohttp->torch-geometric->thesis_dgerber==0.1.0) (2.3.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from aiohttp->torch-geometric->thesis_dgerber==0.1.0) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from aiohttp->torch-geometric->thesis_dgerber==0.1.0) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from aiohttp->torch-geometric->thesis_dgerber==0.1.0) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from aiohttp->torch-geometric->thesis_dgerber==0.1.0) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from aiohttp->torch-geometric->thesis_dgerber==0.1.0) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from aiohttp->torch-geometric->thesis_dgerber==0.1.0) (4.0.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from sympy->torch->thesis_dgerber==0.1.0) (1.3.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /state/partition1/llgrid/pkg/anaconda/python-ML-2024b/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->thesis_dgerber==0.1.0) (5.0.1)\n",
      "Building wheels for collected packages: thesis_dgerber\n",
      "  Building wheel for thesis_dgerber (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for thesis_dgerber: filename=thesis_dgerber-0.1.0-py3-none-any.whl size=1628 sha256=4a9a0ef2ecfc4c559a2f11f783934baf8b10656577cf6100b8963fd684ee25df\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-me370fgx/wheels/be/9b/ab/0a41b82ec761f70f5b0b1c69b0010f7a1287dfacd1e7c84080\n",
      "Successfully built thesis_dgerber\n",
      "Installing collected packages: thesis_dgerber\n",
      "  Attempting uninstall: thesis_dgerber\n",
      "    Found existing installation: thesis_dgerber 0.1.0\n",
      "    Uninstalling thesis_dgerber-0.1.0:\n",
      "      Successfully uninstalled thesis_dgerber-0.1.0\n",
      "Successfully installed thesis_dgerber-0.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install . e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main_model.src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 10:37:43.238767: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-03-25 10:37:43.388600: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-03-25 10:37:43.418111: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-25 10:37:43.701168: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-25 10:37:45.940157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-03-25 10:38:01,657 - INFO - Running decoder training.\n",
      "2025-03-25 10:38:01,674 - INFO - Loaded configuration from {'solver': {'run': 'train', 'logdir': 'main_model/logs/decoder_20', 'max_epoch': 40, 'test_every_epoch': 5, 'save_every_epoch': 5, 'log_per_iter': 1, 'type': 'adam', 'weight_decay': 0.0, 'lr': 0.0001, 'lr_type': 'step', 'milestones': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], 'gamma': 0.9, 'accumulation_steps': 1, 'progress_bar': True, 'rand_seed': 22, 'empty_cache': True, 'ckpt': None, 'clip_grad': 0.0, 'wandb': {'project_name': 'decoder_20'}}, 'data': {'train': {'mode': 'train', 'batch_size': 16, 'num_workers': 4, 'episodes': -1, 'shuffle': True, 'env': {'data_path': 'main_model/disk/problems/mesh_cvrp_data_train_20.h5', 'sub_path': True}, 'disable': False}, 'test': {'mode': 'test', 'batch_size': 1, 'num_workers': 4, 'episodes': -1, 'shuffle': False, 'env': {'data_path': 'main_model/disk/problems/mesh_cvrp_data_test_20.h5', 'sub_path': False}, 'disable': False}}, 'model': {'embedding_dim': 128, 'sqrt_embedding_dim': 11.313708498984761, 'decoder_layer_num': 6, 'qkv_dim': 16, 'head_num': 8, 'ff_hidden_dim': 512, 'pretrained_encoder': {'in_channels': 6, 'hidden_channels': 256, 'out_channels': 32, 'ckp_path': 'main_model/logs/encoder_32/checkpoints/08200.solver.tar', 'mesh_path': 'main_model/disk/meshes/sphere.obj'}}}:\n",
      "solver:\n",
      "  run: train\n",
      "  logdir: main_model/logs/decoder_20\n",
      "  max_epoch: 40\n",
      "  test_every_epoch: 5\n",
      "  save_every_epoch: 5\n",
      "  log_per_iter: 1\n",
      "  type: adam\n",
      "  weight_decay: 0.0\n",
      "  lr: 0.0001\n",
      "  lr_type: step\n",
      "  milestones:\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "  - 6\n",
      "  - 7\n",
      "  - 8\n",
      "  - 9\n",
      "  - 10\n",
      "  - 11\n",
      "  - 12\n",
      "  - 13\n",
      "  - 14\n",
      "  - 15\n",
      "  - 16\n",
      "  - 17\n",
      "  - 18\n",
      "  - 19\n",
      "  - 20\n",
      "  - 21\n",
      "  - 22\n",
      "  - 23\n",
      "  - 24\n",
      "  - 25\n",
      "  - 26\n",
      "  - 27\n",
      "  - 28\n",
      "  - 29\n",
      "  - 30\n",
      "  - 31\n",
      "  - 32\n",
      "  - 33\n",
      "  - 34\n",
      "  - 35\n",
      "  - 36\n",
      "  - 37\n",
      "  - 38\n",
      "  - 39\n",
      "  gamma: 0.9\n",
      "  accumulation_steps: 1\n",
      "  progress_bar: true\n",
      "  rand_seed: 22\n",
      "  empty_cache: true\n",
      "  ckpt: null\n",
      "  clip_grad: 0.0\n",
      "  wandb:\n",
      "    project_name: decoder_20\n",
      "data:\n",
      "  train:\n",
      "    mode: train\n",
      "    batch_size: 16\n",
      "    num_workers: 4\n",
      "    episodes: -1\n",
      "    shuffle: true\n",
      "    env:\n",
      "      data_path: main_model/disk/problems/mesh_cvrp_data_train_20.h5\n",
      "      sub_path: true\n",
      "    disable: false\n",
      "  test:\n",
      "    mode: test\n",
      "    batch_size: 1\n",
      "    num_workers: 4\n",
      "    episodes: -1\n",
      "    shuffle: false\n",
      "    env:\n",
      "      data_path: main_model/disk/problems/mesh_cvrp_data_test_20.h5\n",
      "      sub_path: false\n",
      "    disable: false\n",
      "model:\n",
      "  embedding_dim: 128\n",
      "  sqrt_embedding_dim: 11.313708498984761\n",
      "  decoder_layer_num: 6\n",
      "  qkv_dim: 16\n",
      "  head_num: 8\n",
      "  ff_hidden_dim: 512\n",
      "  pretrained_encoder:\n",
      "    in_channels: 6\n",
      "    hidden_channels: 256\n",
      "    out_channels: 32\n",
      "    ckp_path: main_model/logs/encoder_32/checkpoints/08200.solver.tar\n",
      "    mesh_path: main_model/disk/meshes/sphere.obj\n",
      "\n",
      "2025-03-25 10:38:01,675 - INFO - slurm configs: None, None\n",
      "2025-03-25 10:38:01,675 - INFO - wandb logging: True\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 10:38:03,590 - INFO - wandb project name: decoder_20\n",
      "2025-03-25 10:38:03,591 - INFO - wandb run name: None\n",
      "2025-03-25 10:38:03,592 - INFO - wandb dir: main_model/logs/decoder_20\n",
      "2025-03-25 10:39:22,029 - INFO - LEHD(\n",
      "  (encoder): Encoder(\n",
      "    (pretrained_gegnn): PretrainedGeGnn(\n",
      "      (model): GraphUNet(\n",
      "        (conv1): GraphConvBnRelu(\n",
      "          (conv): MyConv(\n",
      "            (conv): MyConvOp()\n",
      "          )\n",
      "          (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "          (relu): ReLU()\n",
      "        )\n",
      "        (downsample): ModuleList(\n",
      "          (0-4): 5 x PoolingGraph()\n",
      "        )\n",
      "        (encoder): ModuleList(\n",
      "          (0): GraphResBlocks(\n",
      "            (resblks): ModuleList(\n",
      "              (0-1): 2 x GraphResBlock2(\n",
      "                (conv3x3a): GraphConvBnRelu(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 64, eps=0.001, affine=True)\n",
      "                  (relu): ReLU()\n",
      "                )\n",
      "                (conv3x3b): GraphConvBn(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                )\n",
      "                (relu): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-3): 3 x GraphResBlocks(\n",
      "            (resblks): ModuleList(\n",
      "              (0-2): 3 x GraphResBlock2(\n",
      "                (conv3x3a): GraphConvBnRelu(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 64, eps=0.001, affine=True)\n",
      "                  (relu): ReLU()\n",
      "                )\n",
      "                (conv3x3b): GraphConvBn(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                )\n",
      "                (relu): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (4): GraphResBlocks(\n",
      "            (resblks): ModuleList(\n",
      "              (0-1): 2 x GraphResBlock2(\n",
      "                (conv3x3a): GraphConvBnRelu(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 64, eps=0.001, affine=True)\n",
      "                  (relu): ReLU()\n",
      "                )\n",
      "                (conv3x3b): GraphConvBn(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                )\n",
      "                (relu): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (upsample): ModuleList(\n",
      "          (0-4): 5 x UnpoolingGraph()\n",
      "        )\n",
      "        (decoder): ModuleList(\n",
      "          (0): GraphResBlocks(\n",
      "            (resblks): ModuleList(\n",
      "              (0): GraphResBlock2(\n",
      "                (conv3x3a): GraphConvBnRelu(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                  (relu): ReLU()\n",
      "                )\n",
      "                (conv3x3b): GraphConvBn(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                )\n",
      "                (conv1x1): Conv1x1Bn(\n",
      "                  (conv): Conv1x1(\n",
      "                    (linear): Linear(in_features=512, out_features=256, bias=False)\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                )\n",
      "                (relu): ReLU(inplace=True)\n",
      "              )\n",
      "              (1): GraphResBlock2(\n",
      "                (conv3x3a): GraphConvBnRelu(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                  (relu): ReLU()\n",
      "                )\n",
      "                (conv3x3b): GraphConvBn(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                )\n",
      "                (relu): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (1-3): 3 x GraphResBlocks(\n",
      "            (resblks): ModuleList(\n",
      "              (0): GraphResBlock2(\n",
      "                (conv3x3a): GraphConvBnRelu(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                  (relu): ReLU()\n",
      "                )\n",
      "                (conv3x3b): GraphConvBn(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                )\n",
      "                (conv1x1): Conv1x1Bn(\n",
      "                  (conv): Conv1x1(\n",
      "                    (linear): Linear(in_features=512, out_features=256, bias=False)\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                )\n",
      "                (relu): ReLU(inplace=True)\n",
      "              )\n",
      "              (1-2): 2 x GraphResBlock2(\n",
      "                (conv3x3a): GraphConvBnRelu(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                  (relu): ReLU()\n",
      "                )\n",
      "                (conv3x3b): GraphConvBn(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                )\n",
      "                (relu): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "          (4): GraphResBlocks(\n",
      "            (resblks): ModuleList(\n",
      "              (0): GraphResBlock2(\n",
      "                (conv3x3a): GraphConvBnRelu(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                  (relu): ReLU()\n",
      "                )\n",
      "                (conv3x3b): GraphConvBn(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                )\n",
      "                (conv1x1): Conv1x1Bn(\n",
      "                  (conv): Conv1x1(\n",
      "                    (linear): Linear(in_features=512, out_features=256, bias=False)\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                )\n",
      "                (relu): ReLU(inplace=True)\n",
      "              )\n",
      "              (1): GraphResBlock2(\n",
      "                (conv3x3a): GraphConvBnRelu(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                  (relu): ReLU()\n",
      "                )\n",
      "                (conv3x3b): GraphConvBn(\n",
      "                  (conv): MyConv(\n",
      "                    (conv): MyConvOp()\n",
      "                  )\n",
      "                  (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "                )\n",
      "                (relu): ReLU(inplace=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (header): Sequential(\n",
      "          (0): Conv1x1BnRelu(\n",
      "            (conv): Conv1x1(\n",
      "              (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "            )\n",
      "            (bn): GroupNorm(4, 256, eps=0.001, affine=True)\n",
      "            (relu): ReLU(inplace=True)\n",
      "          )\n",
      "          (1): Conv1x1(\n",
      "            (linear): Linear(in_features=256, out_features=32, bias=True)\n",
      "          )\n",
      "        )\n",
      "        (embedding_decoder_mlp): Sequential(\n",
      "          (0): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (1): ReLU()\n",
      "          (2): Linear(in_features=32, out_features=32, bias=True)\n",
      "          (3): ReLU()\n",
      "          (4): Linear(in_features=32, out_features=1, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (transition_layer): Linear(in_features=33, out_features=128, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (Wk): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (Wv): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (multi_head_combine): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (feedForward): Feed_Forward_Module(\n",
      "          (W1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (W2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding_first_node): Linear(in_features=129, out_features=128, bias=True)\n",
      "    (embedding_last_node): Linear(in_features=129, out_features=128, bias=True)\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x DecoderLayer(\n",
      "        (Wq): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (Wk): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (Wv): Linear(in_features=128, out_features=128, bias=False)\n",
      "        (multi_head_combine): Linear(in_features=128, out_features=128, bias=True)\n",
      "        (feedForward): Feed_Forward_Module(\n",
      "          (W1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (W2): Linear(in_features=512, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (Linear_final): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "2025-03-25 10:39:22,036 - INFO - Start loading train dataset from HDF5 file...\n",
      "2025-03-25 10:39:22,249 - INFO - Loading 1000 problems from main_model/disk/problems/mesh_cvrp_data_train_20.h5...\n",
      "Loading train data: 100%|██████████| 1000/1000 [00:00<00:00, 1615.66it/s]\n",
      "2025-03-25 10:39:24,631 - INFO - Loading train dataset done! Loaded 1000 problems.\n",
      "2025-03-25 10:39:24,639 - INFO - Loaded mesh data with 21 city points\n",
      "2025-03-25 10:39:27,017 - INFO - Start loading test dataset from HDF5 file...\n",
      "2025-03-25 10:39:27,026 - INFO - Loading 100 problems from main_model/disk/problems/mesh_cvrp_data_test_20.h5...\n",
      "Loading test data: 100%|██████████| 100/100 [00:00<00:00, 1847.47it/s]\n",
      "2025-03-25 10:39:28,637 - INFO - Loading test dataset done! Loaded 100 problems.\n",
      "2025-03-25 10:39:28,645 - INFO - Loaded mesh data with 21 city points\n",
      "2025-03-25 10:39:30,628 - INFO - logdir: main_model/logs/decoder_20\n",
      "2025-03-25 10:39:30,655 - INFO - ====================  EPOCH   1/ 40  ====================\n",
      "2025-03-25 10:40:25,826 - INFO - Epoch   1: Train   1/ 63 (  1.6%) Loss: 1.0258 Time: 0.92\n",
      "2025-03-25 10:40:30,620 - INFO - Epoch   1: Train   2/ 63 (  3.2%) Loss: 0.5740 Time: 0.08\n",
      "2025-03-25 10:40:32,817 - INFO - Epoch   1: Train   3/ 63 (  4.8%) Loss: 0.4835 Time: 0.04\n",
      "2025-03-25 10:40:34,719 - INFO - Epoch   1: Train   4/ 63 (  6.3%) Loss: 0.4420 Time: 0.03\n",
      "2025-03-25 10:40:36,428 - INFO - Epoch   1: Train   5/ 63 (  7.9%) Loss: 0.4294 Time: 0.03\n",
      "2025-03-25 10:40:38,025 - INFO - Epoch   1: Train   6/ 63 (  9.5%) Loss: 0.4035 Time: 0.03\n",
      "2025-03-25 10:40:40,125 - INFO - Epoch   1: Train   7/ 63 ( 11.1%) Loss: 0.5860 Time: 0.04\n",
      "2025-03-25 10:40:42,019 - INFO - Epoch   1: Train   8/ 63 ( 12.7%) Loss: 0.5640 Time: 0.03\n",
      "2025-03-25 10:40:43,822 - INFO - Epoch   1: Train   9/ 63 ( 14.3%) Loss: 0.3264 Time: 0.03\n",
      "2025-03-25 10:40:46,019 - INFO - Epoch   1: Train  10/ 63 ( 15.9%) Loss: 0.5280 Time: 0.04\n",
      "2025-03-25 10:40:48,223 - INFO - Epoch   1: Train  11/ 63 ( 17.5%) Loss: 0.4771 Time: 0.04\n",
      "2025-03-25 10:40:50,216 - INFO - Epoch   1: Train  12/ 63 ( 19.0%) Loss: 0.4942 Time: 0.03\n",
      "2025-03-25 10:40:52,424 - INFO - Epoch   1: Train  13/ 63 ( 20.6%) Loss: 0.5570 Time: 0.04\n",
      "2025-03-25 10:40:53,925 - INFO - Epoch   1: Train  14/ 63 ( 22.2%) Loss: 0.3544 Time: 0.03\n",
      "2025-03-25 10:40:55,325 - INFO - Epoch   1: Train  15/ 63 ( 23.8%) Loss: 0.3287 Time: 0.02\n",
      "2025-03-25 10:40:56,724 - INFO - Epoch   1: Train  16/ 63 ( 25.4%) Loss: 0.2942 Time: 0.02\n",
      "2025-03-25 10:40:58,425 - INFO - Epoch   1: Train  17/ 63 ( 27.0%) Loss: 0.3215 Time: 0.03\n",
      "2025-03-25 10:41:01,016 - INFO - Epoch   1: Train  18/ 63 ( 28.6%) Loss: 0.6571 Time: 0.04\n",
      "2025-03-25 10:41:03,222 - INFO - Epoch   1: Train  19/ 63 ( 30.2%) Loss: 0.4720 Time: 0.04\n",
      "2025-03-25 10:41:04,625 - INFO - Epoch   1: Train  20/ 63 ( 31.7%) Loss: 0.2632 Time: 0.02\n",
      "2025-03-25 10:41:06,020 - INFO - Epoch   1: Train  21/ 63 ( 33.3%) Loss: 0.2740 Time: 0.02\n",
      "2025-03-25 10:41:07,423 - INFO - Epoch   1: Train  22/ 63 ( 34.9%) Loss: 0.2140 Time: 0.02\n",
      "2025-03-25 10:41:08,823 - INFO - Epoch   1: Train  23/ 63 ( 36.5%) Loss: 0.3769 Time: 0.02\n",
      "2025-03-25 10:41:10,721 - INFO - Epoch   1: Train  24/ 63 ( 38.1%) Loss: 0.5105 Time: 0.03\n",
      "2025-03-25 10:41:12,121 - INFO - Epoch   1: Train  25/ 63 ( 39.7%) Loss: 0.3011 Time: 0.02\n",
      "2025-03-25 10:41:14,021 - INFO - Epoch   1: Train  26/ 63 ( 41.3%) Loss: 0.5349 Time: 0.03\n",
      "2025-03-25 10:41:15,922 - INFO - Epoch   1: Train  27/ 63 ( 42.9%) Loss: 0.5238 Time: 0.03\n",
      "2025-03-25 10:41:18,223 - INFO - Epoch   1: Train  28/ 63 ( 44.4%) Loss: 0.4780 Time: 0.04\n",
      "2025-03-25 10:41:20,218 - INFO - Epoch   1: Train  29/ 63 ( 46.0%) Loss: 0.4923 Time: 0.03\n",
      "2025-03-25 10:41:21,616 - INFO - Epoch   1: Train  30/ 63 ( 47.6%) Loss: 0.2442 Time: 0.02\n",
      "2025-03-25 10:41:22,820 - INFO - Epoch   1: Train  31/ 63 ( 49.2%) Loss: 0.2249 Time: 0.02\n",
      "2025-03-25 10:41:24,921 - INFO - Epoch   1: Train  32/ 63 ( 50.8%) Loss: 0.4357 Time: 0.04\n",
      "2025-03-25 10:41:26,820 - INFO - Epoch   1: Train  33/ 63 ( 52.4%) Loss: 0.5270 Time: 0.03\n",
      "2025-03-25 10:41:28,624 - INFO - Epoch   1: Train  34/ 63 ( 54.0%) Loss: 0.4891 Time: 0.03\n",
      "2025-03-25 10:41:29,924 - INFO - Epoch   1: Train  35/ 63 ( 55.6%) Loss: 0.2598 Time: 0.02\n",
      "2025-03-25 10:41:31,522 - INFO - Epoch   1: Train  36/ 63 ( 57.1%) Loss: 0.3872 Time: 0.03\n",
      "2025-03-25 10:41:32,823 - INFO - Epoch   1: Train  37/ 63 ( 58.7%) Loss: 0.2405 Time: 0.02\n",
      "2025-03-25 10:41:35,122 - INFO - Epoch   1: Train  38/ 63 ( 60.3%) Loss: 0.2018 Time: 0.04\n",
      "2025-03-25 10:41:36,326 - INFO - Epoch   1: Train  39/ 63 ( 61.9%) Loss: 0.2568 Time: 0.02\n",
      "2025-03-25 10:41:38,220 - INFO - Epoch   1: Train  40/ 63 ( 63.5%) Loss: 0.5517 Time: 0.03\n",
      "2025-03-25 10:41:39,521 - INFO - Epoch   1: Train  41/ 63 ( 65.1%) Loss: 0.1680 Time: 0.02\n",
      "2025-03-25 10:41:41,720 - INFO - Epoch   1: Train  42/ 63 ( 66.7%) Loss: 0.5554 Time: 0.04\n",
      "2025-03-25 10:41:43,723 - INFO - Epoch   1: Train  43/ 63 ( 68.3%) Loss: 0.5235 Time: 0.03\n",
      "2025-03-25 10:41:45,425 - INFO - Epoch   1: Train  44/ 63 ( 69.8%) Loss: 0.7164 Time: 0.03\n",
      "2025-03-25 10:41:47,220 - INFO - Epoch   1: Train  45/ 63 ( 71.4%) Loss: 0.4305 Time: 0.03\n",
      "2025-03-25 10:41:48,919 - INFO - Epoch   1: Train  46/ 63 ( 73.0%) Loss: 0.2585 Time: 0.03\n",
      "2025-03-25 10:41:50,520 - INFO - Epoch   1: Train  47/ 63 ( 74.6%) Loss: 0.2118 Time: 0.03\n",
      "2025-03-25 10:41:52,425 - INFO - Epoch   1: Train  48/ 63 ( 76.2%) Loss: 0.6071 Time: 0.03\n",
      "2025-03-25 10:41:53,916 - INFO - Epoch   1: Train  49/ 63 ( 77.8%) Loss: 0.2623 Time: 0.02\n",
      "2025-03-25 10:41:55,826 - INFO - Epoch   1: Train  50/ 63 ( 79.4%) Loss: 0.3972 Time: 0.03\n",
      "2025-03-25 10:41:57,822 - INFO - Epoch   1: Train  51/ 63 ( 81.0%) Loss: 0.4168 Time: 0.03\n",
      "2025-03-25 10:41:59,426 - INFO - Epoch   1: Train  52/ 63 ( 82.5%) Loss: 0.3401 Time: 0.03\n",
      "2025-03-25 10:42:00,922 - INFO - Epoch   1: Train  53/ 63 ( 84.1%) Loss: 0.2976 Time: 0.02\n",
      "2025-03-25 10:42:02,518 - INFO - Epoch   1: Train  54/ 63 ( 85.7%) Loss: 0.2124 Time: 0.03\n",
      "2025-03-25 10:42:03,821 - INFO - Epoch   1: Train  55/ 63 ( 87.3%) Loss: 0.1939 Time: 0.02\n",
      "2025-03-25 10:42:05,625 - INFO - Epoch   1: Train  56/ 63 ( 88.9%) Loss: 0.4536 Time: 0.03\n",
      "2025-03-25 10:42:07,521 - INFO - Epoch   1: Train  57/ 63 ( 90.5%) Loss: 0.4222 Time: 0.03\n",
      "2025-03-25 10:42:09,022 - INFO - Epoch   1: Train  58/ 63 ( 92.1%) Loss: 0.2135 Time: 0.03\n",
      "2025-03-25 10:42:10,424 - INFO - Epoch   1: Train  59/ 63 ( 93.7%) Loss: 0.1925 Time: 0.02\n",
      "2025-03-25 10:42:12,225 - INFO - Epoch   1: Train  60/ 63 ( 95.2%) Loss: 0.2992 Time: 0.03\n",
      "2025-03-25 10:42:13,925 - INFO - Epoch   1: Train  61/ 63 ( 96.8%) Loss: 0.4601 Time: 0.03\n",
      "2025-03-25 10:42:15,923 - INFO - Epoch   1: Train  62/ 63 ( 98.4%) Loss: 0.3843 Time: 0.03\n",
      "2025-03-25 10:42:17,516 - INFO - Epoch   1: Train  63/ 63 (100.0%) Loss: 0.4356 Time: 0.03\n",
      "2025-03-25 10:42:17,518 - INFO -  \n",
      "2025-03-25 10:42:17,519 - INFO - *** Summary ***\n",
      "2025-03-25 10:42:17,520 - INFO - Avg. Loss: 0.41 Avg. Time: 0.04 min\n",
      "2025-03-25 10:42:17,523 - INFO - Elapsed: 2.78 min, Remaining: 1.81 hours\n",
      "2025-03-25 10:42:17,523 - INFO -  \n",
      "2025-03-25 10:42:17,524 - INFO - ====================  EPOCH   2/ 40  ====================\n",
      "2025-03-25 10:42:19,220 - INFO - Epoch   2: Train   1/ 63 (  1.6%) Loss: 0.2124 Time: 0.03\n",
      "2025-03-25 10:42:21,022 - INFO - Epoch   2: Train   2/ 63 (  3.2%) Loss: 0.4246 Time: 0.03\n",
      "2025-03-25 10:42:23,220 - INFO - Epoch   2: Train   3/ 63 (  4.8%) Loss: 0.2502 Time: 0.04\n",
      "2025-03-25 10:42:24,524 - INFO - Epoch   2: Train   4/ 63 (  6.3%) Loss: 0.1581 Time: 0.02\n",
      "2025-03-25 10:42:26,425 - INFO - Epoch   2: Train   5/ 63 (  7.9%) Loss: 0.3392 Time: 0.03\n",
      "2025-03-25 10:42:27,727 - INFO - Epoch   2: Train   6/ 63 (  9.5%) Loss: 0.1026 Time: 0.02\n",
      "2025-03-25 10:42:29,217 - INFO - Epoch   2: Train   7/ 63 ( 11.1%) Loss: 0.1875 Time: 0.02\n",
      "2025-03-25 10:42:30,522 - INFO - Epoch   2: Train   8/ 63 ( 12.7%) Loss: 0.1306 Time: 0.02\n",
      "2025-03-25 10:42:32,421 - INFO - Epoch   2: Train   9/ 63 ( 14.3%) Loss: 0.3032 Time: 0.03\n",
      "2025-03-25 10:42:33,923 - INFO - Epoch   2: Train  10/ 63 ( 15.9%) Loss: 0.1898 Time: 0.03\n",
      "2025-03-25 10:42:35,325 - INFO - Epoch   2: Train  11/ 63 ( 17.5%) Loss: 0.1137 Time: 0.02\n",
      "2025-03-25 10:42:37,121 - INFO - Epoch   2: Train  12/ 63 ( 19.0%) Loss: 0.3007 Time: 0.03\n",
      "2025-03-25 10:42:38,925 - INFO - Epoch   2: Train  13/ 63 ( 20.6%) Loss: 0.1451 Time: 0.03\n",
      "2025-03-25 10:42:40,926 - INFO - Epoch   2: Train  14/ 63 ( 22.2%) Loss: 0.1988 Time: 0.03\n",
      "2025-03-25 10:42:43,121 - INFO - Epoch   2: Train  15/ 63 ( 23.8%) Loss: 0.2343 Time: 0.04\n",
      "2025-03-25 10:42:44,523 - INFO - Epoch   2: Train  16/ 63 ( 25.4%) Loss: 0.1019 Time: 0.02\n",
      "2025-03-25 10:42:46,422 - INFO - Epoch   2: Train  17/ 63 ( 27.0%) Loss: 0.3941 Time: 0.03\n",
      "2025-03-25 10:42:48,221 - INFO - Epoch   2: Train  18/ 63 ( 28.6%) Loss: 0.1497 Time: 0.03\n",
      "2025-03-25 10:42:50,023 - INFO - Epoch   2: Train  19/ 63 ( 30.2%) Loss: 0.1393 Time: 0.03\n",
      "2025-03-25 10:42:51,816 - INFO - Epoch   2: Train  20/ 63 ( 31.7%) Loss: 0.2209 Time: 0.03\n",
      "2025-03-25 10:42:53,124 - INFO - Epoch   2: Train  21/ 63 ( 33.3%) Loss: 0.2029 Time: 0.02\n",
      "2025-03-25 10:42:54,421 - INFO - Epoch   2: Train  22/ 63 ( 34.9%) Loss: 0.0907 Time: 0.02\n",
      "2025-03-25 10:42:56,521 - INFO - Epoch   2: Train  23/ 63 ( 36.5%) Loss: 0.2432 Time: 0.04\n",
      "2025-03-25 10:42:57,821 - INFO - Epoch   2: Train  24/ 63 ( 38.1%) Loss: 0.0869 Time: 0.02\n",
      "2025-03-25 10:42:59,520 - INFO - Epoch   2: Train  25/ 63 ( 39.7%) Loss: 0.2193 Time: 0.03\n",
      "2025-03-25 10:43:00,925 - INFO - Epoch   2: Train  26/ 63 ( 41.3%) Loss: 0.0971 Time: 0.02\n",
      "2025-03-25 10:43:02,519 - INFO - Epoch   2: Train  27/ 63 ( 42.9%) Loss: 0.0688 Time: 0.03\n",
      "2025-03-25 10:43:04,128 - INFO - Epoch   2: Train  28/ 63 ( 44.4%) Loss: 0.1161 Time: 0.03\n",
      "2025-03-25 10:43:05,526 - INFO - Epoch   2: Train  29/ 63 ( 46.0%) Loss: 0.0678 Time: 0.02\n",
      "2025-03-25 10:43:07,423 - INFO - Epoch   2: Train  30/ 63 ( 47.6%) Loss: 0.2905 Time: 0.03\n",
      "2025-03-25 10:43:09,423 - INFO - Epoch   2: Train  31/ 63 ( 49.2%) Loss: 0.1618 Time: 0.03\n",
      "2025-03-25 10:43:11,917 - INFO - Epoch   2: Train  32/ 63 ( 50.8%) Loss: 0.2134 Time: 0.04\n",
      "2025-03-25 10:43:14,026 - INFO - Epoch   2: Train  33/ 63 ( 52.4%) Loss: 0.0935 Time: 0.04\n",
      "2025-03-25 10:43:15,817 - INFO - Epoch   2: Train  34/ 63 ( 54.0%) Loss: 0.0411 Time: 0.03\n",
      "2025-03-25 10:43:17,823 - INFO - Epoch   2: Train  35/ 63 ( 55.6%) Loss: 0.3498 Time: 0.03\n",
      "2025-03-25 10:43:20,016 - INFO - Epoch   2: Train  36/ 63 ( 57.1%) Loss: 0.2452 Time: 0.04\n",
      "2025-03-25 10:43:21,522 - INFO - Epoch   2: Train  37/ 63 ( 58.7%) Loss: 0.1005 Time: 0.03\n",
      "2025-03-25 10:43:23,020 - INFO - Epoch   2: Train  38/ 63 ( 60.3%) Loss: 0.0990 Time: 0.02\n",
      "2025-03-25 10:43:24,425 - INFO - Epoch   2: Train  39/ 63 ( 61.9%) Loss: 0.0934 Time: 0.02\n",
      "2025-03-25 10:43:25,923 - INFO - Epoch   2: Train  40/ 63 ( 63.5%) Loss: 0.0647 Time: 0.02\n",
      "2025-03-25 10:43:27,321 - INFO - Epoch   2: Train  41/ 63 ( 65.1%) Loss: 0.1208 Time: 0.02\n",
      "2025-03-25 10:43:28,622 - INFO - Epoch   2: Train  42/ 63 ( 66.7%) Loss: 0.0839 Time: 0.02\n",
      "2025-03-25 10:43:29,823 - INFO - Epoch   2: Train  43/ 63 ( 68.3%) Loss: 0.1261 Time: 0.02\n",
      "2025-03-25 10:43:31,420 - INFO - Epoch   2: Train  44/ 63 ( 69.8%) Loss: 0.1949 Time: 0.03\n",
      "2025-03-25 10:43:32,726 - INFO - Epoch   2: Train  45/ 63 ( 71.4%) Loss: 0.0490 Time: 0.02\n",
      "2025-03-25 10:43:34,322 - INFO - Epoch   2: Train  46/ 63 ( 73.0%) Loss: 0.0540 Time: 0.03\n",
      "2025-03-25 10:43:36,220 - INFO - Epoch   2: Train  47/ 63 ( 74.6%) Loss: 0.3032 Time: 0.03\n",
      "2025-03-25 10:43:37,721 - INFO - Epoch   2: Train  48/ 63 ( 76.2%) Loss: 0.0215 Time: 0.03\n",
      "2025-03-25 10:43:39,421 - INFO - Epoch   2: Train  49/ 63 ( 77.8%) Loss: 0.2262 Time: 0.03\n",
      "2025-03-25 10:43:40,919 - INFO - Epoch   2: Train  50/ 63 ( 79.4%) Loss: 0.0675 Time: 0.02\n",
      "2025-03-25 10:43:43,020 - INFO - Epoch   2: Train  51/ 63 ( 81.0%) Loss: 0.1890 Time: 0.04\n",
      "2025-03-25 10:43:45,021 - INFO - Epoch   2: Train  52/ 63 ( 82.5%) Loss: 0.1249 Time: 0.03\n",
      "2025-03-25 10:43:46,822 - INFO - Epoch   2: Train  53/ 63 ( 84.1%) Loss: 0.2038 Time: 0.03\n",
      "2025-03-25 10:43:48,225 - INFO - Epoch   2: Train  54/ 63 ( 85.7%) Loss: 0.0751 Time: 0.02\n",
      "2025-03-25 10:43:49,620 - INFO - Epoch   2: Train  55/ 63 ( 87.3%) Loss: 0.0469 Time: 0.02\n",
      "2025-03-25 10:43:51,523 - INFO - Epoch   2: Train  56/ 63 ( 88.9%) Loss: 0.1357 Time: 0.03\n",
      "2025-03-25 10:43:52,927 - INFO - Epoch   2: Train  57/ 63 ( 90.5%) Loss: 0.0806 Time: 0.02\n",
      "2025-03-25 10:43:54,320 - INFO - Epoch   2: Train  58/ 63 ( 92.1%) Loss: 0.1548 Time: 0.02\n",
      "2025-03-25 10:43:55,522 - INFO - Epoch   2: Train  59/ 63 ( 93.7%) Loss: 0.1139 Time: 0.02\n",
      "2025-03-25 10:43:57,621 - INFO - Epoch   2: Train  60/ 63 ( 95.2%) Loss: 0.1200 Time: 0.03\n",
      "2025-03-25 10:43:59,023 - INFO - Epoch   2: Train  61/ 63 ( 96.8%) Loss: 0.0588 Time: 0.02\n",
      "2025-03-25 10:44:01,422 - INFO - Epoch   2: Train  62/ 63 ( 98.4%) Loss: 0.2651 Time: 0.04\n",
      "2025-03-25 10:44:03,123 - INFO - Epoch   2: Train  63/ 63 (100.0%) Loss: 0.3240 Time: 0.03\n",
      "2025-03-25 10:44:03,126 - INFO -  \n",
      "2025-03-25 10:44:03,127 - INFO - *** Summary ***\n",
      "2025-03-25 10:44:03,127 - INFO - Avg. Loss: 0.16 Avg. Time: 0.03 min\n",
      "2025-03-25 10:44:03,129 - INFO - Elapsed: 4.54 min, Remaining: 1.44 hours\n",
      "2025-03-25 10:44:03,217 - INFO -  \n",
      "2025-03-25 10:44:03,217 - INFO - ====================  EPOCH   3/ 40  ====================\n",
      "2025-03-25 10:44:05,125 - INFO - Epoch   3: Train   1/ 63 (  1.6%) Loss: 0.1618 Time: 0.03\n",
      "2025-03-25 10:44:06,820 - INFO - Epoch   3: Train   2/ 63 (  3.2%) Loss: 0.0779 Time: 0.03\n",
      "2025-03-25 10:44:08,723 - INFO - Epoch   3: Train   3/ 63 (  4.8%) Loss: 0.1287 Time: 0.03\n",
      "2025-03-25 10:44:10,117 - INFO - Epoch   3: Train   4/ 63 (  6.3%) Loss: 0.0772 Time: 0.02\n",
      "2025-03-25 10:44:11,922 - INFO - Epoch   3: Train   5/ 63 (  7.9%) Loss: 0.1072 Time: 0.03\n",
      "2025-03-25 10:44:13,625 - INFO - Epoch   3: Train   6/ 63 (  9.5%) Loss: 0.1265 Time: 0.03\n",
      "2025-03-25 10:44:15,627 - INFO - Epoch   3: Train   7/ 63 ( 11.1%) Loss: 0.1963 Time: 0.03\n",
      "2025-03-25 10:44:17,216 - INFO - Epoch   3: Train   8/ 63 ( 12.7%) Loss: 0.0703 Time: 0.02\n",
      "2025-03-25 10:44:19,122 - INFO - Epoch   3: Train   9/ 63 ( 14.3%) Loss: 0.1382 Time: 0.03\n",
      "2025-03-25 10:44:20,821 - INFO - Epoch   3: Train  10/ 63 ( 15.9%) Loss: 0.1837 Time: 0.03\n",
      "2025-03-25 10:44:22,321 - INFO - Epoch   3: Train  11/ 63 ( 17.5%) Loss: 0.1066 Time: 0.03\n",
      "2025-03-25 10:44:24,223 - INFO - Epoch   3: Train  12/ 63 ( 19.0%) Loss: 0.2036 Time: 0.03\n",
      "2025-03-25 10:44:25,621 - INFO - Epoch   3: Train  13/ 63 ( 20.6%) Loss: 0.0762 Time: 0.02\n",
      "2025-03-25 10:44:27,419 - INFO - Epoch   3: Train  14/ 63 ( 22.2%) Loss: 0.0640 Time: 0.03\n",
      "2025-03-25 10:44:28,720 - INFO - Epoch   3: Train  15/ 63 ( 23.8%) Loss: 0.0176 Time: 0.02\n",
      "2025-03-25 10:44:30,319 - INFO - Epoch   3: Train  16/ 63 ( 25.4%) Loss: 0.0628 Time: 0.03\n",
      "2025-03-25 10:44:32,020 - INFO - Epoch   3: Train  17/ 63 ( 27.0%) Loss: 0.2020 Time: 0.03\n",
      "2025-03-25 10:44:33,517 - INFO - Epoch   3: Train  18/ 63 ( 28.6%) Loss: 0.0604 Time: 0.02\n",
      "2025-03-25 10:44:35,319 - INFO - Epoch   3: Train  19/ 63 ( 30.2%) Loss: 0.0863 Time: 0.03\n",
      "2025-03-25 10:44:36,725 - INFO - Epoch   3: Train  20/ 63 ( 31.7%) Loss: 0.0467 Time: 0.02\n",
      "2025-03-25 10:44:38,222 - INFO - Epoch   3: Train  21/ 63 ( 33.3%) Loss: 0.1120 Time: 0.02\n",
      "2025-03-25 10:44:39,718 - INFO - Epoch   3: Train  22/ 63 ( 34.9%) Loss: 0.0540 Time: 0.02\n",
      "2025-03-25 10:44:41,324 - INFO - Epoch   3: Train  23/ 63 ( 36.5%) Loss: 0.2633 Time: 0.03\n",
      "2025-03-25 10:44:43,023 - INFO - Epoch   3: Train  24/ 63 ( 38.1%) Loss: 0.0644 Time: 0.03\n",
      "2025-03-25 10:44:44,716 - INFO - Epoch   3: Train  25/ 63 ( 39.7%) Loss: 0.0362 Time: 0.03\n",
      "2025-03-25 10:44:47,021 - INFO - Epoch   3: Train  26/ 63 ( 41.3%) Loss: 0.1579 Time: 0.04\n",
      "2025-03-25 10:44:49,026 - INFO - Epoch   3: Train  27/ 63 ( 42.9%) Loss: 0.2290 Time: 0.03\n",
      "2025-03-25 10:44:51,224 - INFO - Epoch   3: Train  28/ 63 ( 44.4%) Loss: 0.1323 Time: 0.04\n",
      "2025-03-25 10:44:53,021 - INFO - Epoch   3: Train  29/ 63 ( 46.0%) Loss: 0.1293 Time: 0.03\n",
      "2025-03-25 10:44:54,521 - INFO - Epoch   3: Train  30/ 63 ( 47.6%) Loss: 0.1162 Time: 0.02\n",
      "2025-03-25 10:44:55,923 - INFO - Epoch   3: Train  31/ 63 ( 49.2%) Loss: 0.1346 Time: 0.02\n",
      "2025-03-25 10:44:58,023 - INFO - Epoch   3: Train  32/ 63 ( 50.8%) Loss: 0.0597 Time: 0.04\n",
      "2025-03-25 10:44:59,925 - INFO - Epoch   3: Train  33/ 63 ( 52.4%) Loss: 0.1284 Time: 0.03\n",
      "2025-03-25 10:45:01,721 - INFO - Epoch   3: Train  34/ 63 ( 54.0%) Loss: 0.0524 Time: 0.03\n",
      "2025-03-25 10:45:03,526 - INFO - Epoch   3: Train  35/ 63 ( 55.6%) Loss: 0.0820 Time: 0.03\n",
      "2025-03-25 10:45:05,225 - INFO - Epoch   3: Train  36/ 63 ( 57.1%) Loss: 0.0492 Time: 0.03\n",
      "2025-03-25 10:45:06,717 - INFO - Epoch   3: Train  37/ 63 ( 58.7%) Loss: 0.0645 Time: 0.02\n",
      "2025-03-25 10:45:08,925 - INFO - Epoch   3: Train  38/ 63 ( 60.3%) Loss: 0.1063 Time: 0.04\n",
      "2025-03-25 10:45:11,120 - INFO - Epoch   3: Train  39/ 63 ( 61.9%) Loss: 0.0872 Time: 0.04\n",
      "2025-03-25 10:45:12,620 - INFO - Epoch   3: Train  40/ 63 ( 63.5%) Loss: 0.1834 Time: 0.02\n",
      "2025-03-25 10:45:14,725 - INFO - Epoch   3: Train  41/ 63 ( 65.1%) Loss: 0.1761 Time: 0.04\n",
      "2025-03-25 10:45:16,621 - INFO - Epoch   3: Train  42/ 63 ( 66.7%) Loss: 0.1480 Time: 0.03\n",
      "2025-03-25 10:45:18,525 - INFO - Epoch   3: Train  43/ 63 ( 68.3%) Loss: 0.1752 Time: 0.03\n",
      "2025-03-25 10:45:20,818 - INFO - Epoch   3: Train  44/ 63 ( 69.8%) Loss: 0.1443 Time: 0.04\n",
      "2025-03-25 10:45:22,222 - INFO - Epoch   3: Train  45/ 63 ( 71.4%) Loss: 0.0408 Time: 0.02\n",
      "2025-03-25 10:45:23,819 - INFO - Epoch   3: Train  46/ 63 ( 73.0%) Loss: 0.0146 Time: 0.03\n",
      "2025-03-25 10:45:25,121 - INFO - Epoch   3: Train  47/ 63 ( 74.6%) Loss: 0.1016 Time: 0.02\n",
      "2025-03-25 10:45:26,619 - INFO - Epoch   3: Train  48/ 63 ( 76.2%) Loss: 0.1418 Time: 0.02\n",
      "2025-03-25 10:45:28,123 - INFO - Epoch   3: Train  49/ 63 ( 77.8%) Loss: 0.1048 Time: 0.03\n",
      "2025-03-25 10:45:30,717 - INFO - Epoch   3: Train  50/ 63 ( 79.4%) Loss: 0.1588 Time: 0.04\n",
      "2025-03-25 10:45:32,618 - INFO - Epoch   3: Train  51/ 63 ( 81.0%) Loss: 0.1763 Time: 0.03\n",
      "2025-03-25 10:45:34,422 - INFO - Epoch   3: Train  52/ 63 ( 82.5%) Loss: 0.1090 Time: 0.03\n",
      "2025-03-25 10:45:36,021 - INFO - Epoch   3: Train  53/ 63 ( 84.1%) Loss: 0.0898 Time: 0.03\n",
      "2025-03-25 10:45:38,421 - INFO - Epoch   3: Train  54/ 63 ( 85.7%) Loss: 0.2510 Time: 0.04\n",
      "2025-03-25 10:45:40,020 - INFO - Epoch   3: Train  55/ 63 ( 87.3%) Loss: 0.0386 Time: 0.03\n",
      "2025-03-25 10:45:42,625 - INFO - Epoch   3: Train  56/ 63 ( 88.9%) Loss: 0.2003 Time: 0.04\n",
      "2025-03-25 10:45:44,026 - INFO - Epoch   3: Train  57/ 63 ( 90.5%) Loss: 0.0866 Time: 0.02\n",
      "2025-03-25 10:45:46,017 - INFO - Epoch   3: Train  58/ 63 ( 92.1%) Loss: 0.0429 Time: 0.03\n",
      "2025-03-25 10:45:48,122 - INFO - Epoch   3: Train  59/ 63 ( 93.7%) Loss: 0.1680 Time: 0.04\n",
      "2025-03-25 10:45:49,821 - INFO - Epoch   3: Train  60/ 63 ( 95.2%) Loss: 0.0260 Time: 0.03\n",
      "2025-03-25 10:45:51,717 - INFO - Epoch   3: Train  61/ 63 ( 96.8%) Loss: 0.1237 Time: 0.03\n",
      "2025-03-25 10:45:53,525 - INFO - Epoch   3: Train  62/ 63 ( 98.4%) Loss: 0.1157 Time: 0.03\n",
      "2025-03-25 10:45:55,324 - INFO - Epoch   3: Train  63/ 63 (100.0%) Loss: 0.1496 Time: 0.03\n",
      "2025-03-25 10:45:55,326 - INFO -  \n",
      "2025-03-25 10:45:55,326 - INFO - *** Summary ***\n",
      "2025-03-25 10:45:55,416 - INFO - Avg. Loss: 0.11 Avg. Time: 0.03 min\n",
      "2025-03-25 10:45:55,419 - INFO - Elapsed: 6.41 min, Remaining: 1.32 hours\n",
      "2025-03-25 10:45:55,420 - INFO -  \n",
      "2025-03-25 10:45:55,421 - INFO - ====================  EPOCH   4/ 40  ====================\n",
      "2025-03-25 10:45:57,125 - INFO - Epoch   4: Train   1/ 63 (  1.6%) Loss: 0.0645 Time: 0.03\n",
      "2025-03-25 10:45:59,216 - INFO - Epoch   4: Train   2/ 63 (  3.2%) Loss: 0.0563 Time: 0.03\n",
      "2025-03-25 10:46:01,621 - INFO - Epoch   4: Train   3/ 63 (  4.8%) Loss: 0.1396 Time: 0.04\n",
      "2025-03-25 10:46:03,325 - INFO - Epoch   4: Train   4/ 63 (  6.3%) Loss: 0.0660 Time: 0.03\n",
      "2025-03-25 10:46:06,022 - INFO - Epoch   4: Train   5/ 63 (  7.9%) Loss: 0.1276 Time: 0.04\n",
      "2025-03-25 10:46:08,227 - INFO - Epoch   4: Train   6/ 63 (  9.5%) Loss: 0.2130 Time: 0.04\n",
      "2025-03-25 10:46:10,821 - INFO - Epoch   4: Train   7/ 63 ( 11.1%) Loss: 0.1823 Time: 0.04\n",
      "2025-03-25 10:46:12,624 - INFO - Epoch   4: Train   8/ 63 ( 12.7%) Loss: 0.1830 Time: 0.03\n",
      "2025-03-25 10:46:14,121 - INFO - Epoch   4: Train   9/ 63 ( 14.3%) Loss: 0.0684 Time: 0.02\n",
      "2025-03-25 10:46:16,222 - INFO - Epoch   4: Train  10/ 63 ( 15.9%) Loss: 0.1239 Time: 0.04\n",
      "2025-03-25 10:46:17,622 - INFO - Epoch   4: Train  11/ 63 ( 17.5%) Loss: 0.1655 Time: 0.02\n",
      "2025-03-25 10:46:19,521 - INFO - Epoch   4: Train  12/ 63 ( 19.0%) Loss: 0.1090 Time: 0.03\n",
      "2025-03-25 10:46:21,120 - INFO - Epoch   4: Train  13/ 63 ( 20.6%) Loss: 0.0657 Time: 0.03\n",
      "2025-03-25 10:46:22,717 - INFO - Epoch   4: Train  14/ 63 ( 22.2%) Loss: 0.0210 Time: 0.03\n",
      "2025-03-25 10:46:24,421 - INFO - Epoch   4: Train  15/ 63 ( 23.8%) Loss: 0.1388 Time: 0.03\n",
      "2025-03-25 10:46:26,321 - INFO - Epoch   4: Train  16/ 63 ( 25.4%) Loss: 0.2291 Time: 0.03\n",
      "2025-03-25 10:46:29,223 - INFO - Epoch   4: Train  17/ 63 ( 27.0%) Loss: 0.1048 Time: 0.05\n",
      "2025-03-25 10:46:31,323 - INFO - Epoch   4: Train  18/ 63 ( 28.6%) Loss: 0.1434 Time: 0.04\n",
      "2025-03-25 10:46:33,122 - INFO - Epoch   4: Train  19/ 63 ( 30.2%) Loss: 0.0202 Time: 0.03\n",
      "2025-03-25 10:46:35,025 - INFO - Epoch   4: Train  20/ 63 ( 31.7%) Loss: 0.0206 Time: 0.03\n",
      "2025-03-25 10:46:37,221 - INFO - Epoch   4: Train  21/ 63 ( 33.3%) Loss: 0.0581 Time: 0.04\n",
      "2025-03-25 10:46:38,725 - INFO - Epoch   4: Train  22/ 63 ( 34.9%) Loss: 0.0497 Time: 0.03\n",
      "2025-03-25 10:46:40,120 - INFO - Epoch   4: Train  23/ 63 ( 36.5%) Loss: 0.0979 Time: 0.02\n",
      "2025-03-25 10:46:41,616 - INFO - Epoch   4: Train  24/ 63 ( 38.1%) Loss: 0.0643 Time: 0.02\n",
      "2025-03-25 10:46:43,023 - INFO - Epoch   4: Train  25/ 63 ( 39.7%) Loss: 0.1317 Time: 0.02\n",
      "2025-03-25 10:46:44,917 - INFO - Epoch   4: Train  26/ 63 ( 41.3%) Loss: 0.0086 Time: 0.03\n",
      "2025-03-25 10:46:47,721 - INFO - Epoch   4: Train  27/ 63 ( 42.9%) Loss: 0.0648 Time: 0.05\n",
      "2025-03-25 10:46:49,217 - INFO - Epoch   4: Train  28/ 63 ( 44.4%) Loss: 0.0945 Time: 0.02\n",
      "2025-03-25 10:46:50,721 - INFO - Epoch   4: Train  29/ 63 ( 46.0%) Loss: 0.0381 Time: 0.03\n",
      "2025-03-25 10:46:52,824 - INFO - Epoch   4: Train  30/ 63 ( 47.6%) Loss: 0.0302 Time: 0.04\n",
      "2025-03-25 10:46:55,524 - INFO - Epoch   4: Train  31/ 63 ( 49.2%) Loss: 0.1558 Time: 0.05\n",
      "2025-03-25 10:46:57,925 - INFO - Epoch   4: Train  32/ 63 ( 50.8%) Loss: 0.1013 Time: 0.04\n",
      "2025-03-25 10:46:59,423 - INFO - Epoch   4: Train  33/ 63 ( 52.4%) Loss: 0.0778 Time: 0.02\n",
      "2025-03-25 10:47:01,321 - INFO - Epoch   4: Train  34/ 63 ( 54.0%) Loss: 0.2114 Time: 0.03\n",
      "2025-03-25 10:47:03,724 - INFO - Epoch   4: Train  35/ 63 ( 55.6%) Loss: 0.0786 Time: 0.04\n",
      "2025-03-25 10:47:05,726 - INFO - Epoch   4: Train  36/ 63 ( 57.1%) Loss: 0.1868 Time: 0.03\n",
      "2025-03-25 10:47:07,621 - INFO - Epoch   4: Train  37/ 63 ( 58.7%) Loss: 0.1409 Time: 0.03\n",
      "2025-03-25 10:47:09,319 - INFO - Epoch   4: Train  38/ 63 ( 60.3%) Loss: 0.0598 Time: 0.03\n",
      "2025-03-25 10:47:10,820 - INFO - Epoch   4: Train  39/ 63 ( 61.9%) Loss: 0.0731 Time: 0.03\n",
      "2025-03-25 10:47:13,221 - INFO - Epoch   4: Train  40/ 63 ( 63.5%) Loss: 0.0914 Time: 0.04\n",
      "2025-03-25 10:47:15,123 - INFO - Epoch   4: Train  41/ 63 ( 65.1%) Loss: 0.1292 Time: 0.03\n",
      "2025-03-25 10:47:16,626 - INFO - Epoch   4: Train  42/ 63 ( 66.7%) Loss: 0.0498 Time: 0.03\n",
      "2025-03-25 10:47:18,523 - INFO - Epoch   4: Train  43/ 63 ( 68.3%) Loss: 0.1652 Time: 0.03\n",
      "2025-03-25 10:47:19,917 - INFO - Epoch   4: Train  44/ 63 ( 69.8%) Loss: 0.0560 Time: 0.02\n",
      "2025-03-25 10:47:21,521 - INFO - Epoch   4: Train  45/ 63 ( 71.4%) Loss: 0.0503 Time: 0.03\n",
      "2025-03-25 10:47:23,024 - INFO - Epoch   4: Train  46/ 63 ( 73.0%) Loss: 0.1080 Time: 0.03\n",
      "2025-03-25 10:47:24,621 - INFO - Epoch   4: Train  47/ 63 ( 74.6%) Loss: 0.0523 Time: 0.03\n",
      "2025-03-25 10:47:26,625 - INFO - Epoch   4: Train  48/ 63 ( 76.2%) Loss: 0.1123 Time: 0.03\n",
      "2025-03-25 10:47:29,519 - INFO - Epoch   4: Train  49/ 63 ( 77.8%) Loss: 0.1877 Time: 0.05\n",
      "2025-03-25 10:47:32,117 - INFO - Epoch   4: Train  50/ 63 ( 79.4%) Loss: 0.1574 Time: 0.04\n",
      "2025-03-25 10:47:33,824 - INFO - Epoch   4: Train  51/ 63 ( 81.0%) Loss: 0.0358 Time: 0.03\n",
      "2025-03-25 10:47:36,526 - INFO - Epoch   4: Train  52/ 63 ( 82.5%) Loss: 0.0372 Time: 0.05\n",
      "2025-03-25 10:47:38,722 - INFO - Epoch   4: Train  53/ 63 ( 84.1%) Loss: 0.1228 Time: 0.04\n",
      "2025-03-25 10:47:40,418 - INFO - Epoch   4: Train  54/ 63 ( 85.7%) Loss: 0.0701 Time: 0.03\n",
      "2025-03-25 10:47:42,117 - INFO - Epoch   4: Train  55/ 63 ( 87.3%) Loss: 0.0266 Time: 0.03\n",
      "2025-03-25 10:47:43,923 - INFO - Epoch   4: Train  56/ 63 ( 88.9%) Loss: 0.1861 Time: 0.03\n",
      "2025-03-25 10:47:46,223 - INFO - Epoch   4: Train  57/ 63 ( 90.5%) Loss: 0.1106 Time: 0.04\n",
      "2025-03-25 10:47:48,523 - INFO - Epoch   4: Train  58/ 63 ( 92.1%) Loss: 0.1249 Time: 0.04\n",
      "2025-03-25 10:47:50,121 - INFO - Epoch   4: Train  59/ 63 ( 93.7%) Loss: 0.1023 Time: 0.03\n",
      "2025-03-25 10:47:52,321 - INFO - Epoch   4: Train  60/ 63 ( 95.2%) Loss: 0.1232 Time: 0.04\n",
      "2025-03-25 10:47:54,422 - INFO - Epoch   4: Train  61/ 63 ( 96.8%) Loss: 0.1909 Time: 0.04\n",
      "2025-03-25 10:47:55,921 - INFO - Epoch   4: Train  62/ 63 ( 98.4%) Loss: 0.0471 Time: 0.02\n",
      "2025-03-25 10:47:57,622 - INFO - Epoch   4: Train  63/ 63 (100.0%) Loss: 0.0500 Time: 0.03\n",
      "2025-03-25 10:47:57,624 - INFO -  \n",
      "2025-03-25 10:47:57,625 - INFO - *** Summary ***\n",
      "2025-03-25 10:47:57,626 - INFO - Avg. Loss: 0.10 Avg. Time: 0.03 min\n",
      "2025-03-25 10:47:57,718 - INFO - Elapsed: 8.45 min, Remaining: 1.27 hours\n",
      "2025-03-25 10:47:57,719 - INFO -  \n",
      "2025-03-25 10:47:57,719 - INFO - ====================  EPOCH   5/ 40  ====================\n",
      "2025-03-25 10:47:59,720 - INFO - Epoch   5: Train   1/ 63 (  1.6%) Loss: 0.1312 Time: 0.03\n",
      "2025-03-25 10:48:01,122 - INFO - Epoch   5: Train   2/ 63 (  3.2%) Loss: 0.0970 Time: 0.02\n",
      "2025-03-25 10:48:02,822 - INFO - Epoch   5: Train   3/ 63 (  4.8%) Loss: 0.0395 Time: 0.03\n",
      "2025-03-25 10:48:04,321 - INFO - Epoch   5: Train   4/ 63 (  6.3%) Loss: 0.0529 Time: 0.02\n",
      "2025-03-25 10:48:05,818 - INFO - Epoch   5: Train   5/ 63 (  7.9%) Loss: 0.1052 Time: 0.02\n",
      "2025-03-25 10:48:07,623 - INFO - Epoch   5: Train   6/ 63 (  9.5%) Loss: 0.3342 Time: 0.03\n",
      "2025-03-25 10:48:09,426 - INFO - Epoch   5: Train   7/ 63 ( 11.1%) Loss: 0.0501 Time: 0.03\n",
      "2025-03-25 10:48:11,022 - INFO - Epoch   5: Train   8/ 63 ( 12.7%) Loss: 0.0720 Time: 0.03\n",
      "2025-03-25 10:48:12,921 - INFO - Epoch   5: Train   9/ 63 ( 14.3%) Loss: 0.1054 Time: 0.03\n",
      "2025-03-25 10:48:14,922 - INFO - Epoch   5: Train  10/ 63 ( 15.9%) Loss: 0.1081 Time: 0.03\n",
      "2025-03-25 10:48:16,424 - INFO - Epoch   5: Train  11/ 63 ( 17.5%) Loss: 0.0512 Time: 0.03\n",
      "2025-03-25 10:48:18,017 - INFO - Epoch   5: Train  12/ 63 ( 19.0%) Loss: 0.0107 Time: 0.03\n",
      "2025-03-25 10:48:19,619 - INFO - Epoch   5: Train  13/ 63 ( 20.6%) Loss: 0.0282 Time: 0.03\n",
      "2025-03-25 10:48:22,120 - INFO - Epoch   5: Train  14/ 63 ( 22.2%) Loss: 0.2409 Time: 0.04\n",
      "2025-03-25 10:48:24,122 - INFO - Epoch   5: Train  15/ 63 ( 23.8%) Loss: 0.0990 Time: 0.03\n",
      "2025-03-25 10:48:25,923 - INFO - Epoch   5: Train  16/ 63 ( 25.4%) Loss: 0.0208 Time: 0.03\n",
      "2025-03-25 10:48:28,325 - INFO - Epoch   5: Train  17/ 63 ( 27.0%) Loss: 0.1314 Time: 0.04\n",
      "2025-03-25 10:48:30,426 - INFO - Epoch   5: Train  18/ 63 ( 28.6%) Loss: 0.1402 Time: 0.04\n",
      "2025-03-25 10:48:32,725 - INFO - Epoch   5: Train  19/ 63 ( 30.2%) Loss: 0.1437 Time: 0.04\n",
      "2025-03-25 10:48:34,516 - INFO - Epoch   5: Train  20/ 63 ( 31.7%) Loss: 0.0902 Time: 0.03\n",
      "2025-03-25 10:48:36,725 - INFO - Epoch   5: Train  21/ 63 ( 33.3%) Loss: 0.1529 Time: 0.04\n",
      "2025-03-25 10:48:38,218 - INFO - Epoch   5: Train  22/ 63 ( 34.9%) Loss: 0.0940 Time: 0.02\n",
      "2025-03-25 10:48:40,422 - INFO - Epoch   5: Train  23/ 63 ( 36.5%) Loss: 0.1303 Time: 0.04\n",
      "2025-03-25 10:48:42,724 - INFO - Epoch   5: Train  24/ 63 ( 38.1%) Loss: 0.1625 Time: 0.04\n",
      "2025-03-25 10:48:45,021 - INFO - Epoch   5: Train  25/ 63 ( 39.7%) Loss: 0.0691 Time: 0.04\n",
      "2025-03-25 10:48:46,527 - INFO - Epoch   5: Train  26/ 63 ( 41.3%) Loss: 0.0261 Time: 0.03\n",
      "2025-03-25 10:48:48,021 - INFO - Epoch   5: Train  27/ 63 ( 42.9%) Loss: 0.0362 Time: 0.02\n",
      "2025-03-25 10:48:49,821 - INFO - Epoch   5: Train  28/ 63 ( 44.4%) Loss: 0.0779 Time: 0.03\n",
      "2025-03-25 10:48:51,424 - INFO - Epoch   5: Train  29/ 63 ( 46.0%) Loss: 0.0335 Time: 0.03\n",
      "2025-03-25 10:48:53,519 - INFO - Epoch   5: Train  30/ 63 ( 47.6%) Loss: 0.0808 Time: 0.03\n",
      "2025-03-25 10:48:55,422 - INFO - Epoch   5: Train  31/ 63 ( 49.2%) Loss: 0.0325 Time: 0.03\n",
      "2025-03-25 10:48:57,618 - INFO - Epoch   5: Train  32/ 63 ( 50.8%) Loss: 0.0664 Time: 0.04\n",
      "2025-03-25 10:48:59,825 - INFO - Epoch   5: Train  33/ 63 ( 52.4%) Loss: 0.0843 Time: 0.04\n",
      "2025-03-25 10:49:01,324 - INFO - Epoch   5: Train  34/ 63 ( 54.0%) Loss: 0.0213 Time: 0.02\n",
      "2025-03-25 10:49:02,918 - INFO - Epoch   5: Train  35/ 63 ( 55.6%) Loss: 0.1212 Time: 0.03\n",
      "2025-03-25 10:49:04,322 - INFO - Epoch   5: Train  36/ 63 ( 57.1%) Loss: 0.1379 Time: 0.02\n",
      "2025-03-25 10:49:06,320 - INFO - Epoch   5: Train  37/ 63 ( 58.7%) Loss: 0.1150 Time: 0.03\n",
      "2025-03-25 10:49:08,428 - INFO - Epoch   5: Train  38/ 63 ( 60.3%) Loss: 0.0675 Time: 0.04\n",
      "2025-03-25 10:49:10,021 - INFO - Epoch   5: Train  39/ 63 ( 61.9%) Loss: 0.0313 Time: 0.03\n",
      "2025-03-25 10:49:11,423 - INFO - Epoch   5: Train  40/ 63 ( 63.5%) Loss: 0.0078 Time: 0.02\n",
      "2025-03-25 10:49:12,916 - INFO - Epoch   5: Train  41/ 63 ( 65.1%) Loss: 0.0304 Time: 0.02\n",
      "2025-03-25 10:49:14,519 - INFO - Epoch   5: Train  42/ 63 ( 66.7%) Loss: 0.0865 Time: 0.03\n",
      "2025-03-25 10:49:16,721 - INFO - Epoch   5: Train  43/ 63 ( 68.3%) Loss: 0.0458 Time: 0.04\n",
      "2025-03-25 10:49:19,020 - INFO - Epoch   5: Train  44/ 63 ( 69.8%) Loss: 0.1023 Time: 0.04\n",
      "2025-03-25 10:49:21,226 - INFO - Epoch   5: Train  45/ 63 ( 71.4%) Loss: 0.2102 Time: 0.04\n",
      "2025-03-25 10:49:23,427 - INFO - Epoch   5: Train  46/ 63 ( 73.0%) Loss: 0.0550 Time: 0.04\n",
      "2025-03-25 10:49:25,924 - INFO - Epoch   5: Train  47/ 63 ( 74.6%) Loss: 0.1091 Time: 0.04\n",
      "2025-03-25 10:49:27,920 - INFO - Epoch   5: Train  48/ 63 ( 76.2%) Loss: 0.1211 Time: 0.03\n",
      "2025-03-25 10:49:29,921 - INFO - Epoch   5: Train  49/ 63 ( 77.8%) Loss: 0.1434 Time: 0.03\n",
      "2025-03-25 10:49:31,517 - INFO - Epoch   5: Train  50/ 63 ( 79.4%) Loss: 0.0199 Time: 0.03\n",
      "2025-03-25 10:49:33,126 - INFO - Epoch   5: Train  51/ 63 ( 81.0%) Loss: 0.0789 Time: 0.03\n",
      "2025-03-25 10:49:34,727 - INFO - Epoch   5: Train  52/ 63 ( 82.5%) Loss: 0.0813 Time: 0.03\n",
      "2025-03-25 10:49:36,520 - INFO - Epoch   5: Train  53/ 63 ( 84.1%) Loss: 0.0377 Time: 0.03\n",
      "2025-03-25 10:49:38,821 - INFO - Epoch   5: Train  54/ 63 ( 85.7%) Loss: 0.0909 Time: 0.04\n",
      "2025-03-25 10:49:40,519 - INFO - Epoch   5: Train  55/ 63 ( 87.3%) Loss: 0.0602 Time: 0.03\n",
      "2025-03-25 10:49:42,217 - INFO - Epoch   5: Train  56/ 63 ( 88.9%) Loss: 0.0212 Time: 0.03\n",
      "2025-03-25 10:49:44,725 - INFO - Epoch   5: Train  57/ 63 ( 90.5%) Loss: 0.0896 Time: 0.04\n",
      "2025-03-25 10:49:46,723 - INFO - Epoch   5: Train  58/ 63 ( 92.1%) Loss: 0.1988 Time: 0.03\n",
      "2025-03-25 10:49:48,425 - INFO - Epoch   5: Train  59/ 63 ( 93.7%) Loss: 0.0623 Time: 0.03\n",
      "2025-03-25 10:49:50,425 - INFO - Epoch   5: Train  60/ 63 ( 95.2%) Loss: 0.0963 Time: 0.03\n",
      "2025-03-25 10:49:52,522 - INFO - Epoch   5: Train  61/ 63 ( 96.8%) Loss: 0.0526 Time: 0.03\n",
      "2025-03-25 10:49:54,519 - INFO - Epoch   5: Train  62/ 63 ( 98.4%) Loss: 0.0858 Time: 0.03\n",
      "2025-03-25 10:49:56,122 - INFO - Epoch   5: Train  63/ 63 (100.0%) Loss: 0.1225 Time: 0.03\n",
      "2025-03-25 10:49:56,124 - INFO -  \n",
      "2025-03-25 10:49:56,124 - INFO - *** Summary ***\n",
      "2025-03-25 10:49:56,125 - INFO - Avg. Loss: 0.09 Avg. Time: 0.03 min\n",
      "2025-03-25 10:49:56,127 - INFO - Elapsed: 10.42 min, Remaining: 1.22 hours\n",
      "2025-03-25 10:49:56,217 - INFO -  \n",
      "2025-03-25 10:49:56,217 - INFO - -------------------  TESTING   5/ 40  -------------------\n",
      "2025-03-25 10:49:56,523 - INFO - Epoch   5: Test   1/100 (  1.0%) Gap: 2.5183 Time: 0.01\n",
      "2025-03-25 10:49:56,823 - INFO - Epoch   5: Test   2/100 (  2.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:49:57,223 - INFO - Epoch   5: Test   3/100 (  3.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:49:57,716 - INFO - Epoch   5: Test   4/100 (  4.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:49:58,023 - INFO - Epoch   5: Test   5/100 (  5.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:49:58,225 - INFO - Epoch   5: Test   6/100 (  6.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:49:58,523 - INFO - Epoch   5: Test   7/100 (  7.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:49:58,922 - INFO - Epoch   5: Test   8/100 (  8.0%) Gap: 0.4970 Time: 0.01\n",
      "2025-03-25 10:49:59,419 - INFO - Epoch   5: Test   9/100 (  9.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:49:59,624 - INFO - Epoch   5: Test  10/100 ( 10.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:49:59,927 - INFO - Epoch   5: Test  11/100 ( 11.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:00,320 - INFO - Epoch   5: Test  12/100 ( 12.0%) Gap: 4.9105 Time: 0.01\n",
      "2025-03-25 10:50:00,619 - INFO - Epoch   5: Test  13/100 ( 13.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:00,920 - INFO - Epoch   5: Test  14/100 ( 14.0%) Gap: 6.1285 Time: 0.01\n",
      "2025-03-25 10:50:01,226 - INFO - Epoch   5: Test  15/100 ( 15.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:01,624 - INFO - Epoch   5: Test  16/100 ( 16.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:02,018 - INFO - Epoch   5: Test  17/100 ( 17.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:02,422 - INFO - Epoch   5: Test  18/100 ( 18.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:02,722 - INFO - Epoch   5: Test  19/100 ( 19.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:03,016 - INFO - Epoch   5: Test  20/100 ( 20.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:03,316 - INFO - Epoch   5: Test  21/100 ( 21.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:03,623 - INFO - Epoch   5: Test  22/100 ( 22.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:03,925 - INFO - Epoch   5: Test  23/100 ( 23.0%) Gap: 1.3352 Time: 0.01\n",
      "2025-03-25 10:50:04,316 - INFO - Epoch   5: Test  24/100 ( 24.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:04,619 - INFO - Epoch   5: Test  25/100 ( 25.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:04,918 - INFO - Epoch   5: Test  26/100 ( 26.0%) Gap: 19.1578 Time: 0.00\n",
      "2025-03-25 10:50:05,220 - INFO - Epoch   5: Test  27/100 ( 27.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:05,622 - INFO - Epoch   5: Test  28/100 ( 28.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:05,924 - INFO - Epoch   5: Test  29/100 ( 29.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:06,224 - INFO - Epoch   5: Test  30/100 ( 30.0%) Gap: 5.9683 Time: 0.01\n",
      "2025-03-25 10:50:06,617 - INFO - Epoch   5: Test  31/100 ( 31.0%) Gap: 6.1066 Time: 0.01\n",
      "2025-03-25 10:50:06,823 - INFO - Epoch   5: Test  32/100 ( 32.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:07,123 - INFO - Epoch   5: Test  33/100 ( 33.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:07,523 - INFO - Epoch   5: Test  34/100 ( 34.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:07,822 - INFO - Epoch   5: Test  35/100 ( 35.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:08,223 - INFO - Epoch   5: Test  36/100 ( 36.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:08,822 - INFO - Epoch   5: Test  37/100 ( 37.0%) Gap: 8.4707 Time: 0.01\n",
      "2025-03-25 10:50:09,225 - INFO - Epoch   5: Test  38/100 ( 38.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:09,616 - INFO - Epoch   5: Test  39/100 ( 39.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:10,018 - INFO - Epoch   5: Test  40/100 ( 40.0%) Gap: 3.3888 Time: 0.01\n",
      "2025-03-25 10:50:10,227 - INFO - Epoch   5: Test  41/100 ( 41.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:10,525 - INFO - Epoch   5: Test  42/100 ( 42.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:10,825 - INFO - Epoch   5: Test  43/100 ( 43.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:11,034 - INFO - Epoch   5: Test  44/100 ( 44.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:11,325 - INFO - Epoch   5: Test  45/100 ( 45.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:11,625 - INFO - Epoch   5: Test  46/100 ( 46.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:11,927 - INFO - Epoch   5: Test  47/100 ( 47.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:12,225 - INFO - Epoch   5: Test  48/100 ( 48.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:12,622 - INFO - Epoch   5: Test  49/100 ( 49.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:12,931 - INFO - Epoch   5: Test  50/100 ( 50.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:13,223 - INFO - Epoch   5: Test  51/100 ( 51.0%) Gap: 6.0698 Time: 0.00\n",
      "2025-03-25 10:50:13,616 - INFO - Epoch   5: Test  52/100 ( 52.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:13,827 - INFO - Epoch   5: Test  53/100 ( 53.0%) Gap: 6.7259 Time: 0.00\n",
      "2025-03-25 10:50:14,122 - INFO - Epoch   5: Test  54/100 ( 54.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:14,325 - INFO - Epoch   5: Test  55/100 ( 55.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:14,720 - INFO - Epoch   5: Test  56/100 ( 56.0%) Gap: 5.2643 Time: 0.01\n",
      "2025-03-25 10:50:15,023 - INFO - Epoch   5: Test  57/100 ( 57.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:15,323 - INFO - Epoch   5: Test  58/100 ( 58.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:15,721 - INFO - Epoch   5: Test  59/100 ( 59.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:16,025 - INFO - Epoch   5: Test  60/100 ( 60.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:16,326 - INFO - Epoch   5: Test  61/100 ( 61.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:16,625 - INFO - Epoch   5: Test  62/100 ( 62.0%) Gap: 13.6214 Time: 0.00\n",
      "2025-03-25 10:50:17,121 - INFO - Epoch   5: Test  63/100 ( 63.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:17,419 - INFO - Epoch   5: Test  64/100 ( 64.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:17,722 - INFO - Epoch   5: Test  65/100 ( 65.0%) Gap: 8.8037 Time: 0.01\n",
      "2025-03-25 10:50:18,022 - INFO - Epoch   5: Test  66/100 ( 66.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:18,319 - INFO - Epoch   5: Test  67/100 ( 67.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:18,619 - INFO - Epoch   5: Test  68/100 ( 68.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:18,920 - INFO - Epoch   5: Test  69/100 ( 69.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:19,322 - INFO - Epoch   5: Test  70/100 ( 70.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:19,817 - INFO - Epoch   5: Test  71/100 ( 71.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:20,119 - INFO - Epoch   5: Test  72/100 ( 72.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:20,420 - INFO - Epoch   5: Test  73/100 ( 73.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:20,921 - INFO - Epoch   5: Test  74/100 ( 74.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:21,816 - INFO - Epoch   5: Test  75/100 ( 75.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:22,122 - INFO - Epoch   5: Test  76/100 ( 76.0%) Gap: 6.7887 Time: 0.01\n",
      "2025-03-25 10:50:22,424 - INFO - Epoch   5: Test  77/100 ( 77.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:22,725 - INFO - Epoch   5: Test  78/100 ( 78.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:23,025 - INFO - Epoch   5: Test  79/100 ( 79.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:23,418 - INFO - Epoch   5: Test  80/100 ( 80.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:23,731 - INFO - Epoch   5: Test  81/100 ( 81.0%) Gap: 4.3278 Time: 0.01\n",
      "2025-03-25 10:50:24,117 - INFO - Epoch   5: Test  82/100 ( 82.0%) Gap: 11.2609 Time: 0.01\n",
      "2025-03-25 10:50:24,325 - INFO - Epoch   5: Test  83/100 ( 83.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:24,719 - INFO - Epoch   5: Test  84/100 ( 84.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:25,019 - INFO - Epoch   5: Test  85/100 ( 85.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:25,323 - INFO - Epoch   5: Test  86/100 ( 86.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:25,620 - INFO - Epoch   5: Test  87/100 ( 87.0%) Gap: 0.0000 Time: 0.00\n",
      "2025-03-25 10:50:25,922 - INFO - Epoch   5: Test  88/100 ( 88.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:26,224 - INFO - Epoch   5: Test  89/100 ( 89.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:26,524 - INFO - Epoch   5: Test  90/100 ( 90.0%) Gap: 7.6358 Time: 0.00\n",
      "2025-03-25 10:50:27,021 - INFO - Epoch   5: Test  91/100 ( 91.0%) Gap: 4.2353 Time: 0.01\n",
      "2025-03-25 10:50:27,422 - INFO - Epoch   5: Test  92/100 ( 92.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:27,921 - INFO - Epoch   5: Test  93/100 ( 93.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:28,324 - INFO - Epoch   5: Test  94/100 ( 94.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:28,821 - INFO - Epoch   5: Test  95/100 ( 95.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:29,319 - INFO - Epoch   5: Test  96/100 ( 96.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:29,625 - INFO - Epoch   5: Test  97/100 ( 97.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:29,924 - INFO - Epoch   5: Test  98/100 ( 98.0%) Gap: 5.9054 Time: 0.00\n",
      "2025-03-25 10:50:30,231 - INFO - Epoch   5: Test  99/100 ( 99.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:30,619 - INFO - Epoch   5: Test 100/100 (100.0%) Gap: 0.0000 Time: 0.01\n",
      "2025-03-25 10:50:30,620 - INFO -  \n",
      "2025-03-25 10:50:30,621 - INFO - *** Summary ***\n",
      "2025-03-25 10:50:30,623 - INFO - Avg. Opt. Score: 6.55, Avg. St. Score: 6.64\n",
      "2025-03-25 10:50:30,624 - INFO - Avg. Gap: 1.39%, Avg. Time 0.01 min\n",
      "2025-03-25 10:50:30,624 - INFO -  \n",
      "2025-03-25 10:50:30,871 - INFO - Saved checkpoint to main_model/logs/decoder_20/checkpoints\n",
      "2025-03-25 10:50:30,872 - INFO - ====================  EPOCH   6/ 40  ====================\n",
      "2025-03-25 10:50:32,618 - INFO - Epoch   6: Train   1/ 63 (  1.6%) Loss: 0.0755 Time: 0.03\n",
      "2025-03-25 10:50:35,617 - INFO - Epoch   6: Train   2/ 63 (  3.2%) Loss: 0.1580 Time: 0.05\n",
      "2025-03-25 10:50:38,017 - INFO - Epoch   6: Train   3/ 63 (  4.8%) Loss: 0.0944 Time: 0.04\n",
      "2025-03-25 10:50:39,723 - INFO - Epoch   6: Train   4/ 63 (  6.3%) Loss: 0.0257 Time: 0.03\n",
      "2025-03-25 10:50:41,423 - INFO - Epoch   6: Train   5/ 63 (  7.9%) Loss: 0.0912 Time: 0.03\n",
      "2025-03-25 10:50:43,419 - INFO - Epoch   6: Train   6/ 63 (  9.5%) Loss: 0.0613 Time: 0.03\n",
      "2025-03-25 10:50:45,423 - INFO - Epoch   6: Train   7/ 63 ( 11.1%) Loss: 0.0975 Time: 0.03\n",
      "2025-03-25 10:50:47,618 - INFO - Epoch   6: Train   8/ 63 ( 12.7%) Loss: 0.0435 Time: 0.04\n",
      "2025-03-25 10:50:49,719 - INFO - Epoch   6: Train   9/ 63 ( 14.3%) Loss: 0.0566 Time: 0.04\n",
      "2025-03-25 10:50:51,427 - INFO - Epoch   6: Train  10/ 63 ( 15.9%) Loss: 0.1462 Time: 0.03\n",
      "2025-03-25 10:50:53,819 - INFO - Epoch   6: Train  11/ 63 ( 17.5%) Loss: 0.1318 Time: 0.04\n",
      "2025-03-25 10:50:55,325 - INFO - Epoch   6: Train  12/ 63 ( 19.0%) Loss: 0.0644 Time: 0.03\n",
      "2025-03-25 10:50:56,919 - INFO - Epoch   6: Train  13/ 63 ( 20.6%) Loss: 0.0672 Time: 0.03\n",
      "2025-03-25 10:50:58,925 - INFO - Epoch   6: Train  14/ 63 ( 22.2%) Loss: 0.0957 Time: 0.03\n",
      "2025-03-25 10:51:01,224 - INFO - Epoch   6: Train  15/ 63 ( 23.8%) Loss: 0.1200 Time: 0.04\n",
      "2025-03-25 10:51:03,018 - INFO - Epoch   6: Train  16/ 63 ( 25.4%) Loss: 0.0348 Time: 0.03\n",
      "2025-03-25 10:51:04,519 - INFO - Epoch   6: Train  17/ 63 ( 27.0%) Loss: 0.0362 Time: 0.03\n",
      "2025-03-25 10:51:06,522 - INFO - Epoch   6: Train  18/ 63 ( 28.6%) Loss: 0.0726 Time: 0.03\n",
      "2025-03-25 10:51:08,423 - INFO - Epoch   6: Train  19/ 63 ( 30.2%) Loss: 0.1301 Time: 0.03\n",
      "2025-03-25 10:51:10,424 - INFO - Epoch   6: Train  20/ 63 ( 31.7%) Loss: 0.0808 Time: 0.03\n",
      "2025-03-25 10:51:12,323 - INFO - Epoch   6: Train  21/ 63 ( 33.3%) Loss: 0.0275 Time: 0.03\n",
      "2025-03-25 10:51:13,926 - INFO - Epoch   6: Train  22/ 63 ( 34.9%) Loss: 0.1155 Time: 0.03\n",
      "2025-03-25 10:51:15,521 - INFO - Epoch   6: Train  23/ 63 ( 36.5%) Loss: 0.1021 Time: 0.03\n",
      "2025-03-25 10:51:17,623 - INFO - Epoch   6: Train  24/ 63 ( 38.1%) Loss: 0.0870 Time: 0.04\n",
      "2025-03-25 10:51:19,223 - INFO - Epoch   6: Train  25/ 63 ( 39.7%) Loss: 0.0056 Time: 0.03\n",
      "2025-03-25 10:51:20,824 - INFO - Epoch   6: Train  26/ 63 ( 41.3%) Loss: 0.0301 Time: 0.03\n",
      "2025-03-25 10:51:23,017 - INFO - Epoch   6: Train  27/ 63 ( 42.9%) Loss: 0.1176 Time: 0.04\n",
      "2025-03-25 10:51:24,528 - INFO - Epoch   6: Train  28/ 63 ( 44.4%) Loss: 0.0195 Time: 0.03\n",
      "2025-03-25 10:51:26,719 - INFO - Epoch   6: Train  29/ 63 ( 46.0%) Loss: 0.0604 Time: 0.04\n",
      "2025-03-25 10:51:28,618 - INFO - Epoch   6: Train  30/ 63 ( 47.6%) Loss: 0.0796 Time: 0.03\n",
      "2025-03-25 10:51:30,524 - INFO - Epoch   6: Train  31/ 63 ( 49.2%) Loss: 0.0662 Time: 0.03\n",
      "2025-03-25 10:51:32,825 - INFO - Epoch   6: Train  32/ 63 ( 50.8%) Loss: 0.0384 Time: 0.04\n",
      "2025-03-25 10:51:34,924 - INFO - Epoch   6: Train  33/ 63 ( 52.4%) Loss: 0.0982 Time: 0.03\n",
      "2025-03-25 10:51:36,525 - INFO - Epoch   6: Train  34/ 63 ( 54.0%) Loss: 0.0095 Time: 0.03\n",
      "2025-03-25 10:51:38,621 - INFO - Epoch   6: Train  35/ 63 ( 55.6%) Loss: 0.0987 Time: 0.03\n",
      "2025-03-25 10:51:40,122 - INFO - Epoch   6: Train  36/ 63 ( 57.1%) Loss: 0.0491 Time: 0.03\n",
      "2025-03-25 10:51:41,817 - INFO - Epoch   6: Train  37/ 63 ( 58.7%) Loss: 0.0173 Time: 0.03\n",
      "2025-03-25 10:51:44,319 - INFO - Epoch   6: Train  38/ 63 ( 60.3%) Loss: 0.0531 Time: 0.04\n",
      "2025-03-25 10:51:46,720 - INFO - Epoch   6: Train  39/ 63 ( 61.9%) Loss: 0.0644 Time: 0.04\n",
      "2025-03-25 10:51:48,822 - INFO - Epoch   6: Train  40/ 63 ( 63.5%) Loss: 0.0660 Time: 0.04\n",
      "2025-03-25 10:51:50,824 - INFO - Epoch   6: Train  41/ 63 ( 65.1%) Loss: 0.1433 Time: 0.03\n",
      "2025-03-25 10:51:53,025 - INFO - Epoch   6: Train  42/ 63 ( 66.7%) Loss: 0.1457 Time: 0.04\n",
      "2025-03-25 10:51:54,916 - INFO - Epoch   6: Train  43/ 63 ( 68.3%) Loss: 0.0381 Time: 0.03\n",
      "2025-03-25 10:51:57,620 - INFO - Epoch   6: Train  44/ 63 ( 69.8%) Loss: 0.1034 Time: 0.05\n",
      "2025-03-25 10:51:59,823 - INFO - Epoch   6: Train  45/ 63 ( 71.4%) Loss: 0.0372 Time: 0.04\n",
      "2025-03-25 10:52:02,822 - INFO - Epoch   6: Train  46/ 63 ( 73.0%) Loss: 0.0576 Time: 0.05\n",
      "2025-03-25 10:52:05,726 - INFO - Epoch   6: Train  47/ 63 ( 74.6%) Loss: 0.0313 Time: 0.05\n",
      "2025-03-25 10:52:08,326 - INFO - Epoch   6: Train  48/ 63 ( 76.2%) Loss: 0.1069 Time: 0.04\n",
      "2025-03-25 10:52:09,924 - INFO - Epoch   6: Train  49/ 63 ( 77.8%) Loss: 0.0405 Time: 0.03\n",
      "2025-03-25 10:52:11,424 - INFO - Epoch   6: Train  50/ 63 ( 79.4%) Loss: 0.0298 Time: 0.02\n",
      "2025-03-25 10:52:13,422 - INFO - Epoch   6: Train  51/ 63 ( 81.0%) Loss: 0.1195 Time: 0.03\n",
      "2025-03-25 10:52:15,125 - INFO - Epoch   6: Train  52/ 63 ( 82.5%) Loss: 0.0575 Time: 0.03\n",
      "2025-03-25 10:52:16,721 - INFO - Epoch   6: Train  53/ 63 ( 84.1%) Loss: 0.0472 Time: 0.03\n",
      "2025-03-25 10:52:19,116 - INFO - Epoch   6: Train  54/ 63 ( 85.7%) Loss: 0.0268 Time: 0.04\n",
      "2025-03-25 10:52:20,922 - INFO - Epoch   6: Train  55/ 63 ( 87.3%) Loss: 0.2203 Time: 0.03\n",
      "2025-03-25 10:52:23,318 - INFO - Epoch   6: Train  56/ 63 ( 88.9%) Loss: 0.1640 Time: 0.04\n",
      "2025-03-25 10:52:25,226 - INFO - Epoch   6: Train  57/ 63 ( 90.5%) Loss: 0.1402 Time: 0.03\n",
      "2025-03-25 10:52:26,817 - INFO - Epoch   6: Train  58/ 63 ( 92.1%) Loss: 0.0977 Time: 0.03\n",
      "2025-03-25 10:52:28,319 - INFO - Epoch   6: Train  59/ 63 ( 93.7%) Loss: 0.1216 Time: 0.03\n",
      "2025-03-25 10:52:30,116 - INFO - Epoch   6: Train  60/ 63 ( 95.2%) Loss: 0.0458 Time: 0.03\n",
      "2025-03-25 10:52:32,224 - INFO - Epoch   6: Train  61/ 63 ( 96.8%) Loss: 0.1946 Time: 0.04\n",
      "2025-03-25 10:52:33,822 - INFO - Epoch   6: Train  62/ 63 ( 98.4%) Loss: 0.0689 Time: 0.03\n",
      "2025-03-25 10:52:35,324 - INFO - Epoch   6: Train  63/ 63 (100.0%) Loss: 0.1092 Time: 0.03\n",
      "2025-03-25 10:52:35,327 - INFO -  \n",
      "2025-03-25 10:52:35,417 - INFO - *** Summary ***\n",
      "2025-03-25 10:52:35,418 - INFO - Avg. Loss: 0.08 Avg. Time: 0.03 min\n",
      "2025-03-25 10:52:35,420 - INFO - Elapsed: 13.08 min, Remaining: 1.24 hours\n",
      "2025-03-25 10:52:35,421 - INFO -  \n",
      "2025-03-25 10:52:35,422 - INFO - ====================  EPOCH   7/ 40  ====================\n",
      "2025-03-25 10:52:37,320 - INFO - Epoch   7: Train   1/ 63 (  1.6%) Loss: 0.0923 Time: 0.03\n",
      "2025-03-25 10:52:39,425 - INFO - Epoch   7: Train   2/ 63 (  3.2%) Loss: 0.1187 Time: 0.04\n",
      "2025-03-25 10:52:41,818 - INFO - Epoch   7: Train   3/ 63 (  4.8%) Loss: 0.1648 Time: 0.04\n",
      "2025-03-25 10:52:43,820 - INFO - Epoch   7: Train   4/ 63 (  6.3%) Loss: 0.0117 Time: 0.03\n",
      "2025-03-25 10:52:45,824 - INFO - Epoch   7: Train   5/ 63 (  7.9%) Loss: 0.1584 Time: 0.03\n",
      "2025-03-25 10:52:47,427 - INFO - Epoch   7: Train   6/ 63 (  9.5%) Loss: 0.1250 Time: 0.03\n",
      "2025-03-25 10:52:49,324 - INFO - Epoch   7: Train   7/ 63 ( 11.1%) Loss: 0.0310 Time: 0.03\n",
      "2025-03-25 10:52:51,122 - INFO - Epoch   7: Train   8/ 63 ( 12.7%) Loss: 0.0229 Time: 0.03\n",
      "2025-03-25 10:52:54,019 - INFO - Epoch   7: Train   9/ 63 ( 14.3%) Loss: 0.0997 Time: 0.05\n",
      "2025-03-25 10:52:55,926 - INFO - Epoch   7: Train  10/ 63 ( 15.9%) Loss: 0.0184 Time: 0.03\n",
      "2025-03-25 10:52:57,626 - INFO - Epoch   7: Train  11/ 63 ( 17.5%) Loss: 0.0404 Time: 0.03\n",
      "2025-03-25 10:52:59,325 - INFO - Epoch   7: Train  12/ 63 ( 19.0%) Loss: 0.0478 Time: 0.03\n",
      "2025-03-25 10:53:01,419 - INFO - Epoch   7: Train  13/ 63 ( 20.6%) Loss: 0.1613 Time: 0.03\n",
      "2025-03-25 10:53:03,425 - INFO - Epoch   7: Train  14/ 63 ( 22.2%) Loss: 0.1169 Time: 0.03\n",
      "2025-03-25 10:53:05,421 - INFO - Epoch   7: Train  15/ 63 ( 23.8%) Loss: 0.1107 Time: 0.03\n",
      "2025-03-25 10:53:07,525 - INFO - Epoch   7: Train  16/ 63 ( 25.4%) Loss: 0.0739 Time: 0.04\n",
      "2025-03-25 10:53:09,925 - INFO - Epoch   7: Train  17/ 63 ( 27.0%) Loss: 0.1086 Time: 0.04\n",
      "2025-03-25 10:53:12,022 - INFO - Epoch   7: Train  18/ 63 ( 28.6%) Loss: 0.0858 Time: 0.03\n",
      "2025-03-25 10:53:14,223 - INFO - Epoch   7: Train  19/ 63 ( 30.2%) Loss: 0.0565 Time: 0.04\n",
      "2025-03-25 10:53:15,924 - INFO - Epoch   7: Train  20/ 63 ( 31.7%) Loss: 0.0788 Time: 0.03\n",
      "2025-03-25 10:53:18,728 - INFO - Epoch   7: Train  21/ 63 ( 33.3%) Loss: 0.1014 Time: 0.05\n",
      "2025-03-25 10:53:21,524 - INFO - Epoch   7: Train  22/ 63 ( 34.9%) Loss: 0.1171 Time: 0.05\n",
      "2025-03-25 10:53:23,819 - INFO - Epoch   7: Train  23/ 63 ( 36.5%) Loss: 0.1098 Time: 0.04\n",
      "2025-03-25 10:53:26,119 - INFO - Epoch   7: Train  24/ 63 ( 38.1%) Loss: 0.0872 Time: 0.04\n",
      "2025-03-25 10:53:28,923 - INFO - Epoch   7: Train  25/ 63 ( 39.7%) Loss: 0.1079 Time: 0.05\n",
      "2025-03-25 10:53:31,124 - INFO - Epoch   7: Train  26/ 63 ( 41.3%) Loss: 0.0785 Time: 0.04\n",
      "2025-03-25 10:53:32,624 - INFO - Epoch   7: Train  27/ 63 ( 42.9%) Loss: 0.0200 Time: 0.02\n",
      "2025-03-25 10:53:34,124 - INFO - Epoch   7: Train  28/ 63 ( 44.4%) Loss: 0.0820 Time: 0.03\n",
      "2025-03-25 10:53:35,927 - INFO - Epoch   7: Train  29/ 63 ( 46.0%) Loss: 0.0410 Time: 0.03\n",
      "2025-03-25 10:53:37,627 - INFO - Epoch   7: Train  30/ 63 ( 47.6%) Loss: 0.0878 Time: 0.03\n",
      "2025-03-25 10:53:39,720 - INFO - Epoch   7: Train  31/ 63 ( 49.2%) Loss: 0.1910 Time: 0.03\n",
      "2025-03-25 10:53:41,519 - INFO - Epoch   7: Train  32/ 63 ( 50.8%) Loss: 0.0693 Time: 0.03\n",
      "2025-03-25 10:53:43,723 - INFO - Epoch   7: Train  33/ 63 ( 52.4%) Loss: 0.0640 Time: 0.04\n",
      "2025-03-25 10:53:45,723 - INFO - Epoch   7: Train  34/ 63 ( 54.0%) Loss: 0.0394 Time: 0.03\n",
      "2025-03-25 10:53:47,723 - INFO - Epoch   7: Train  35/ 63 ( 55.6%) Loss: 0.0250 Time: 0.03\n",
      "2025-03-25 10:53:49,426 - INFO - Epoch   7: Train  36/ 63 ( 57.1%) Loss: 0.1174 Time: 0.03\n",
      "2025-03-25 10:53:51,223 - INFO - Epoch   7: Train  37/ 63 ( 58.7%) Loss: 0.0451 Time: 0.03\n",
      "2025-03-25 10:53:53,417 - INFO - Epoch   7: Train  38/ 63 ( 60.3%) Loss: 0.1776 Time: 0.04\n",
      "2025-03-25 10:53:55,720 - INFO - Epoch   7: Train  39/ 63 ( 61.9%) Loss: 0.0841 Time: 0.04\n",
      "2025-03-25 10:53:58,121 - INFO - Epoch   7: Train  40/ 63 ( 63.5%) Loss: 0.1044 Time: 0.04\n",
      "2025-03-25 10:53:59,925 - INFO - Epoch   7: Train  41/ 63 ( 65.1%) Loss: 0.0955 Time: 0.03\n",
      "2025-03-25 10:54:03,019 - INFO - Epoch   7: Train  42/ 63 ( 66.7%) Loss: 0.2317 Time: 0.05\n",
      "2025-03-25 10:54:05,521 - INFO - Epoch   7: Train  43/ 63 ( 68.3%) Loss: 0.0511 Time: 0.04\n",
      "2025-03-25 10:54:08,121 - INFO - Epoch   7: Train  44/ 63 ( 69.8%) Loss: 0.0836 Time: 0.04\n",
      "2025-03-25 10:54:10,723 - INFO - Epoch   7: Train  45/ 63 ( 71.4%) Loss: 0.0344 Time: 0.04\n",
      "2025-03-25 10:54:13,417 - INFO - Epoch   7: Train  46/ 63 ( 73.0%) Loss: 0.1392 Time: 0.04\n",
      "2025-03-25 10:54:15,220 - INFO - Epoch   7: Train  47/ 63 ( 74.6%) Loss: 0.0965 Time: 0.03\n",
      "2025-03-25 10:54:17,020 - INFO - Epoch   7: Train  48/ 63 ( 76.2%) Loss: 0.0111 Time: 0.03\n",
      "2025-03-25 10:54:18,624 - INFO - Epoch   7: Train  49/ 63 ( 77.8%) Loss: 0.0935 Time: 0.03\n",
      "2025-03-25 10:54:20,922 - INFO - Epoch   7: Train  50/ 63 ( 79.4%) Loss: 0.1943 Time: 0.04\n",
      "2025-03-25 10:54:22,924 - INFO - Epoch   7: Train  51/ 63 ( 81.0%) Loss: 0.1126 Time: 0.03\n",
      "2025-03-25 10:54:25,117 - INFO - Epoch   7: Train  52/ 63 ( 82.5%) Loss: 0.0624 Time: 0.04\n",
      "2025-03-25 10:54:26,917 - INFO - Epoch   7: Train  53/ 63 ( 84.1%) Loss: 0.0474 Time: 0.03\n",
      "2025-03-25 10:54:29,023 - INFO - Epoch   7: Train  54/ 63 ( 85.7%) Loss: 0.1110 Time: 0.04\n",
      "2025-03-25 10:54:31,425 - INFO - Epoch   7: Train  55/ 63 ( 87.3%) Loss: 0.1190 Time: 0.04\n",
      "2025-03-25 10:54:33,819 - INFO - Epoch   7: Train  56/ 63 ( 88.9%) Loss: 0.0962 Time: 0.04\n",
      "2025-03-25 10:54:35,620 - INFO - Epoch   7: Train  57/ 63 ( 90.5%) Loss: 0.0640 Time: 0.03\n",
      "2025-03-25 10:54:38,219 - INFO - Epoch   7: Train  58/ 63 ( 92.1%) Loss: 0.0228 Time: 0.04\n",
      "2025-03-25 10:54:40,521 - INFO - Epoch   7: Train  59/ 63 ( 93.7%) Loss: 0.0720 Time: 0.04\n",
      "2025-03-25 10:54:42,925 - INFO - Epoch   7: Train  60/ 63 ( 95.2%) Loss: 0.0985 Time: 0.04\n",
      "2025-03-25 10:54:44,521 - INFO - Epoch   7: Train  61/ 63 ( 96.8%) Loss: 0.0314 Time: 0.03\n",
      "2025-03-25 10:54:46,820 - INFO - Epoch   7: Train  62/ 63 ( 98.4%) Loss: 0.0662 Time: 0.04\n",
      "2025-03-25 10:54:48,317 - INFO - Epoch   7: Train  63/ 63 (100.0%) Loss: 0.1347 Time: 0.02\n",
      "2025-03-25 10:54:48,319 - INFO -  \n",
      "2025-03-25 10:54:48,319 - INFO - *** Summary ***\n",
      "2025-03-25 10:54:48,320 - INFO - Avg. Loss: 0.09 Avg. Time: 0.04 min\n",
      "2025-03-25 10:54:48,322 - INFO - Elapsed: 15.29 min, Remaining: 1.20 hours\n",
      "2025-03-25 10:54:48,323 - INFO -  \n",
      "2025-03-25 10:54:48,324 - INFO - ====================  EPOCH   8/ 40  ====================\n",
      "2025-03-25 10:54:51,022 - INFO - Epoch   8: Train   1/ 63 (  1.6%) Loss: 0.0560 Time: 0.04\n",
      "2025-03-25 10:54:53,124 - INFO - Epoch   8: Train   2/ 63 (  3.2%) Loss: 0.0202 Time: 0.04\n",
      "2025-03-25 10:54:55,225 - INFO - Epoch   8: Train   3/ 63 (  4.8%) Loss: 0.0809 Time: 0.04\n",
      "2025-03-25 10:54:57,324 - INFO - Epoch   8: Train   4/ 63 (  6.3%) Loss: 0.0829 Time: 0.03\n",
      "2025-03-25 10:54:59,526 - INFO - Epoch   8: Train   5/ 63 (  7.9%) Loss: 0.0863 Time: 0.04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 59\u001b[0m\n\u001b[1;32m     53\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded configuration from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00myaml\u001b[38;5;241m.\u001b[39mdump(config,\u001b[38;5;250m \u001b[39mdefault_flow_style\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\u001b[38;5;250m \u001b[39msort_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     55\u001b[0m )\n\u001b[1;32m     57\u001b[0m solver \u001b[38;5;241m=\u001b[39m model(config)\n\u001b[0;32m---> 59\u001b[0m \u001b[43msolver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/thesis/thesis_dgerber/main_model/src/trainer/base_trainer.py:484\u001b[0m, in \u001b[0;36mSolver.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;43meval\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mself.\u001b[39;49m\u001b[38;5;132;43;01m%s\u001b[39;49;00m\u001b[38;5;124;43m()\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msolver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m<string>:1\u001b[0m\n",
      "File \u001b[0;32m~/thesis/thesis_dgerber/main_model/src/trainer/base_trainer.py:384\u001b[0m, in \u001b[0;36mSolver.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    380\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m====================  EPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_epoch\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m3d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m  ====================\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    382\u001b[0m )\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# training epoch\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# update learning rate\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/thesis/thesis_dgerber/main_model/src/trainer/decoder_trainer.py:94\u001b[0m, in \u001b[0;36mLEHDTrainer.train_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m     91\u001b[0m elapsed_time[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/data\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m tick])\n\u001b[1;32m     93\u001b[0m \u001b[38;5;66;03m# forward and backward\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# track the averaged tensors\u001b[39;00m\n\u001b[1;32m     97\u001b[0m elapsed_time[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime/batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor([time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m tick])\n",
      "File \u001b[0;32m~/thesis/thesis_dgerber/main_model/src/trainer/decoder_trainer.py:262\u001b[0m, in \u001b[0;36mLEHDTrainer.train_step\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# Backpropagate and update model\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 262\u001b[0m \u001b[43mloss_mean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip_grad_norm()\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/anaconda/python-ML-2024b/lib/python3.10/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda/python-ML-2024b/lib/python3.10/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/anaconda/python-ML-2024b/lib/python3.10/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import yaml\n",
    "import torch\n",
    "\n",
    "from main_model.src.utils.config import load_config, parse_args\n",
    "from main_model.src.trainer.encoder_trainer import GeGnnTrainer\n",
    "from main_model.src.trainer.decoder_trainer import LEHDTrainer\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Set the type of model to run, either \"encoder\" or \"decoder\"\n",
    "TYPE = \"decoder\"\n",
    "\n",
    "\n",
    "####################################################################################################\n",
    "##### Main ########################################################################################\n",
    "####################################################################################################\n",
    "\n",
    "# Configure the logger\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # Set the logging level\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",  # Define the log message format\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),  # Send logs to the console (stdout)\n",
    "        # logging.FileHandler(\"my_log_file.log\"),  # Optionally, write logs to a file\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Define the models\n",
    "models = {\"encoder\": GeGnnTrainer, \"decoder\": LEHDTrainer}\n",
    "paths = {\n",
    "    \"encoder\": \"main_model/configs/config_encoder.yaml\",\n",
    "    \"decoder\": \"main_model/configs/config_decoder.yaml\",\n",
    "}\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    assert (\n",
    "        TYPE in models.keys()\n",
    "    ), f\"Invalid model type: {TYPE}. Must be one of {models.keys()}\"\n",
    "\n",
    "    # Select the model to run\n",
    "    model = models[TYPE]\n",
    "    path = paths[TYPE]\n",
    "\n",
    "    logging.info(f\"Running {TYPE} training.\")\n",
    "\n",
    "    # Load the config file\n",
    "    sys.argv = [sys.argv[0]]\n",
    "    args = parse_args(path=path)\n",
    "    config = load_config(args.config)\n",
    "\n",
    "    # Pretty-print the config using YAML formatting\n",
    "    logging.info(\n",
    "        f\"Loaded configuration from {config}:\\n{yaml.dump(config, default_flow_style=False, sort_keys=False)}\"\n",
    "    )\n",
    "\n",
    "    solver = model(config)\n",
    "\n",
    "    solver.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
